{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Multi Agent Env with Variable-Length Action Spaces in RLlib\n",
    "\n",
    "RLlib on card games:\n",
    "- How to train multiple agents. In particular, every agent (player) should have its own trajectory so that its final reward propagates on his trajectory. Still, all the agents might follow the same policy.\n",
    "    https://docs.ray.io/en/master/rllib-env.html#multi-agent-and-hierarchical \n",
    "- Action space changes depending on the current state. Depending on the cards on the table I might  not be able to play some cards in my hand. In order to mask out some actions:\n",
    "    https://docs.ray.io/en/master/rllib-models.html#variable-length-parametric-action-spaces\n",
    "\n",
    "RLlib can create distinct policies and route agent decisions to its bound policy. When an agent first appears in the env, policy_mapping_fn will be called to determine which policy it is bound to. These assignments are done when the agent first enters the episode, and persist for the duration of the episode.\n",
    "\n",
    "RLlib reports separate training statistics for each policy in the return from train(), along with the combined reward.\n",
    "\n",
    "If all “agents” in the env are homogeneous, then it is possible to use existing single-agent algorithms for training. Since there is still only a single policy being trained, RLlib only needs to internally aggregate the experiences of the different agents prior to policy optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the custom environment\n",
    "https://docs.ray.io/en/latest/rllib-env.html?multi-agent-and-hierarchical#multi-agent-and-hierarchical\n",
    "\n",
    "```python\n",
    "# Example: using a multi-agent env\n",
    "> env = MultiAgentTrafficEnv(num_cars=20, num_traffic_lights=5)\n",
    "\n",
    "# Observations are a dict mapping agent names to their obs. Only those\n",
    "# agents' names that require actions in the next call to `step()` will\n",
    "# be present in the returned observation dict.\n",
    "> print(env.reset())\n",
    "{\n",
    "    \"car_1\": [[...]],\n",
    "    \"car_2\": [[...]],\n",
    "    \"traffic_light_1\": [[...]],\n",
    "}\n",
    "\n",
    "# In the following call to `step`, actions should be provided for each\n",
    "# agent that returned an observation before:\n",
    "> new_obs, rewards, dones, infos = env.step(actions={\"car_1\": ..., \"car_2\": ..., \"traffic_light_1\": ...})\n",
    "\n",
    "# Similarly, new_obs, rewards, dones, etc. also become dicts\n",
    "> print(rewards)\n",
    "{\"car_1\": 3, \"car_2\": -1, \"traffic_light_1\": 0}\n",
    "\n",
    "# Individual agents can early exit; The entire episode is done when \"__all__\" = True\n",
    "> print(dones)\n",
    "{\"car_2\": True, \"__all__\": False}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym.spaces import Discrete, Tuple\n",
    "s = Tuple((\n",
    "    # First round\n",
    "    Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "    # Second round\n",
    "    Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "    # Third round\n",
    "    Tuple((Discrete(4), Discrete(4))),\n",
    "))\n",
    "\n",
    "state = [\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [0, 0]\n",
    "]\n",
    "\n",
    "s.contains(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Multi Agent Env. game\n",
    "\n",
    "from gym.spaces import Discrete, Tuple\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import random\n",
    "\n",
    "\n",
    "class Actions:\n",
    "    # number of actions\n",
    "    SIZE = 3\n",
    "\n",
    "    # types o actions\n",
    "    ROCK = 0\n",
    "    PAPER = 1\n",
    "    SCISSORS = 2\n",
    "    NA = 3  # Not Available, hand not yet played\n",
    "\n",
    "class RockPaperScissors(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Two-player environment for the famous rock paper scissors game, modified:\n",
    "    - There are two agents which alternate, the action of one agent provides the\n",
    "        state for the next agent. Since one of the two players begins, the agent\n",
    "        which starts second should learn to always win! The startign player\n",
    "        is drawn randomly.\n",
    "    - The action space changes. The game is divided in three rounds across\n",
    "        which you can't re-use the same action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Action/State spaces\n",
    "    ACTION_SPACE = Discrete(Actions.SIZE)\n",
    "    \n",
    "    OBSERVATION_SPACE = Tuple((\n",
    "        # First round\n",
    "        Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "        # Second round\n",
    "        Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "        # Third round\n",
    "        Tuple((Discrete(4), Discrete(4))),\n",
    "    ))\n",
    "#     OBSERVATION_SPACE = Dict({\n",
    "#         \"real_obs\": Tuple((\n",
    "#             # First round\n",
    "#             Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "#             # Second round\n",
    "#             Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "#             # Third round\n",
    "#             Tuple((Discrete(4), Discrete(4))),\n",
    "#         )),\n",
    "#         # we have to handle changing action spaces\n",
    "#         \"action_mask\": Box(0, 1, shape=(Actions.SIZE, )),\n",
    "# #         \"avail_actions\": Box(-1, 1, shape=(Actions.SIZE, action_embedding_sz)),\n",
    "#     })\n",
    "    \n",
    "    \n",
    "    # Reward mapping\n",
    "    rewards = {\n",
    "        (Actions.ROCK, Actions.ROCK): (0, 0),\n",
    "        (Actions.ROCK, Actions.PAPER): (-1, 1),\n",
    "        (Actions.ROCK, Actions.SCISSORS): (1, -1),\n",
    "        (Actions.PAPER, Actions.ROCK): (1, -1),\n",
    "        (Actions.PAPER, Actions.PAPER): (0, 0),\n",
    "        (Actions.PAPER, Actions.SCISSORS): (-1, 1),\n",
    "        (Actions.SCISSORS, Actions.ROCK): (-1, 1),\n",
    "        (Actions.SCISSORS, Actions.PAPER): (1, -1),\n",
    "        (Actions.SCISSORS, Actions.SCISSORS): (0, 0),\n",
    "    }\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \n",
    "        # state and action spaces\n",
    "        self.action_space = self.ACTION_SPACE\n",
    "        self.observation_space = self.OBSERVATION_SPACE\n",
    "\n",
    "        self.players = [\"player_1\", \"player_2\"]        \n",
    "\n",
    "    def reset(self):\n",
    "        self.player_scores = [0, 0]  # not used\n",
    "        self.curr_round = 0\n",
    "        self.player_pointer = random.randint(0, 1)\n",
    "        self.state = [\n",
    "            [3, 3],\n",
    "            [3, 3],\n",
    "            [3, 3],\n",
    "        ]\n",
    "        # reward is given to the last player with 1 delay\n",
    "        self.reward_buffer = {p: 0 for p in self.players}\n",
    "        return {self.players[self.player_pointer]: self.state}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # Get current player\n",
    "        curr_player_pointer = self.player_pointer\n",
    "        curr_player = self.players[self.player_pointer]\n",
    "\n",
    "        # Get next player\n",
    "        next_player_pointer = (self.player_pointer + 1) % 2\n",
    "        next_player = self.players[next_player_pointer]\n",
    "    \n",
    "        # Make sure you have the ation only for the current player\n",
    "        assert curr_player in action_dict and len(action_dict) == 1,\\\n",
    "            \"{} should be playing but action {} was received.\".format(curr_player, action_dict)\n",
    "        \n",
    "        # Play the action\n",
    "        curr_action = action_dict[curr_player]\n",
    "        assert self.action_space.contains(curr_action), 'Action {} is not valid'.format(curr_action)\n",
    "        assert self.state[self.curr_round][curr_player_pointer] == self.NA,\\\n",
    "            \"Player {} has already played in round {}. Here the current state: {}\".format(\n",
    "            curr_player_pointer,\n",
    "            self.curr_round,\n",
    "            self.state\n",
    "        )\n",
    "        self.state[self.curr_round][curr_player_pointer] = curr_action\n",
    "\n",
    "        # We might be not done yet\n",
    "        done = {\"__all__\": False}\n",
    "        \n",
    "        # If the next player has already played, the round is done\n",
    "        game_done = False\n",
    "        round_done = self.state[self.curr_round][next_player_pointer] != self.NA\n",
    "        if round_done:\n",
    "            # If the round is done we compute the rewards\n",
    "            curr_rewards = self.rewards[tuple(self.state[self.curr_round])]\n",
    "            self.player_scores[0] += curr_rewards[0]\n",
    "            self.player_scores[1] += curr_rewards[1]            \n",
    "            self.reward_buffer[curr_player] = curr_rewards[curr_player_pointer]\n",
    "            \n",
    "            self.curr_round += 1\n",
    "            if self.curr_round == 3:\n",
    "                done = {\"__all__\": True}\n",
    "                # Return reward and state for all players\n",
    "                reward = self.reward_buffer\n",
    "                obs = {p: self.state for p in self.players}\n",
    "                game_done = True\n",
    "        \n",
    "        # Get the state and reward for the next player\n",
    "        if not game_done:\n",
    "            obs = {next_player: self.state}\n",
    "            reward = {next_player: self.reward_buffer[next_player]}\n",
    "        \n",
    "        # Move pointer to next player\n",
    "        self.player_pointer = next_player_pointer\n",
    "        return obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_2': [[3, 3], [3, 3], [3, 3]]}\n",
      "\n",
      "Round 0: player_2\n",
      "Insert action (0, 1, 2): 0\n",
      "{'player_1': [[3, 0], [3, 3], [3, 3]]} {'player_1': 0} {'__all__': False}\n",
      "\n",
      "Round 0: player_1\n",
      "Insert action (0, 1, 2): 1\n",
      "{'player_2': [[1, 0], [3, 3], [3, 3]]} {'player_2': 0} {'__all__': False}\n",
      "\n",
      "Round 1: player_2\n",
      "Insert action (0, 1, 2): 1\n",
      "{'player_1': [[1, 0], [3, 1], [3, 3]]} {'player_1': 1} {'__all__': False}\n",
      "\n",
      "Round 1: player_1\n",
      "Insert action (0, 1, 2): 0\n",
      "{'player_2': [[1, 0], [0, 1], [3, 3]]} {'player_2': 0} {'__all__': False}\n",
      "\n",
      "Round 2: player_2\n",
      "Insert action (0, 1, 2): 2\n",
      "{'player_1': [[1, 0], [0, 1], [3, 2]]} {'player_1': -1} {'__all__': False}\n",
      "\n",
      "Round 2: player_1\n",
      "Insert action (0, 1, 2): 2\n",
      "{'player_1': [[1, 0], [0, 1], [2, 2]], 'player_2': [[1, 0], [0, 1], [2, 2]]} {'player_1': 0, 'player_2': 0} {'__all__': True}\n"
     ]
    }
   ],
   "source": [
    "# Test the environment\n",
    "import random\n",
    "\n",
    "env = RockPaperScissors()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    print('\\nRound {}: {}'.format(env.curr_round, env.players[env.player_pointer]))\n",
    "    action = {list(obs.keys())[0]: int(input('Insert action (0, 1, 2): '))}\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(obs, reward, done)\n",
    "    is_done = done['__all__']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the custom model for Variable-Length Action Spaces\n",
    "https://docs.ray.io/en/master/rllib-models.html#variable-length-parametric-action-spaces\n",
    "Our policy has to take into consideration the fact that some actions might not be executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-20 20:34:40,114\tINFO resource_spec.py:212 -- Starting Ray with 4.44 GiB memory available for workers and up to 2.24 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-06-20 20:34:40,494\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.125',\n",
       " 'raylet_ip_address': '192.168.1.125',\n",
       " 'redis_address': '192.168.1.125:38515',\n",
       " 'object_store_address': '/tmp/ray/session_2020-06-20_20-34-40_110513_90627/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-06-20_20-34-40_110513_90627/sockets/raylet',\n",
       " 'webui_url': 'localhost:8266',\n",
       " 'session_dir': '/tmp/ray/session_2020-06-20_20-34-40_110513_90627'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before training we have to initialize ray\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Example with tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/4.15 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /home/niccolo/ray_results/RLlibExample<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RockPaperScissors_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         42.7967</td><td style=\"text-align: right;\">36999</td><td style=\"text-align: right;\"> 2.92286</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "config = {\n",
    "    \"env\": RockPaperScissors,\n",
    "#     \"framework\": \"torch\",\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episode_reward_mean\": 2.90,\n",
    "#     \"timesteps_total\": stop_timesteps,\n",
    "#     \"training_iteration\": stop_iters,\n",
    "}\n",
    "\n",
    "results = tune.run(\n",
    "    PPOTrainer,\n",
    "    name='RLlibExample',\n",
    "    config=config,\n",
    "    verbose=1,\n",
    "    stop=stop\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Plain example without tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-20 20:30:01,649\tINFO trainable.py:217 -- Getting current IP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0. episode_reward_mean: 0.04148783977110158\n",
      "Iteration 1. episode_reward_mean: 0.848575712143928\n",
      "Iteration 2. episode_reward_mean: 1.2814285714285714\n",
      "Iteration 3. episode_reward_mean: 1.9835082458770614\n",
      "Iteration 4. episode_reward_mean: 2.3605150214592276\n"
     ]
    }
   ],
   "source": [
    "trainer_config = {\n",
    "    \"env\": RockPaperScissors\n",
    "}\n",
    "trainer = PPOTrainer(config=trainer_config)\n",
    "for i in range(5):\n",
    "    res = trainer.train()\n",
    "    print(\"Iteration {}. episode_reward_mean: {}\".format(i, res['episode_reward_mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Example with multiple policies\n",
    "Inspired from: https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_two_trainers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-20 20:36:55,479\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-20 20:36:58,276\tWARNING trainer_template.py:124 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "2020-06-20 20:36:58,277\tINFO trainable.py:217 -- Getting current IP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Iteration 0 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: -0.05389221556886228\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: -0.025525525525525526\n",
      "== Iteration 1 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.005988023952095809\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 0.2912912912912913\n",
      "== Iteration 2 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.5654761904761905\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 0.7485029940119761\n",
      "== Iteration 3 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.38922155688622756\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 0.972972972972973\n",
      "== Iteration 4 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.6646706586826348\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.021021021021021\n",
      "== Iteration 5 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.9940476190476191\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.2664670658682635\n",
      "== Iteration 6 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.9880239520958084\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.3063063063063063\n",
      "== Iteration 7 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.1197604790419162\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.3198198198198199\n",
      "== Iteration 8 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.4583333333333333\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4296407185628743\n",
      "== Iteration 9 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.3353293413173652\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.490990990990991\n",
      "== Iteration 10 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.622754491017964\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4534534534534536\n",
      "== Iteration 11 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.5416666666666667\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.3158682634730539\n",
      "== Iteration 12 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.8562874251497006\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.3813813813813813\n",
      "== Iteration 13 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.4131736526946108\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4534534534534536\n",
      "== Iteration 14 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.4642857142857142\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.404191616766467\n",
      "== Iteration 15 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.688622754491018\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.454954954954955\n",
      "== Iteration 16 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.473053892215569\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.457957957957958\n",
      "== Iteration 17 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.5714285714285714\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.5074850299401197\n",
      "== Iteration 18 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.532934131736527\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.6486486486486487\n",
      "== Iteration 19 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.467065868263473\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.451951951951952\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"ppo_policy_1\": (PPOTFPolicy, RockPaperScissors.OBSERVATION_SPACE, RockPaperScissors.ACTION_SPACE, {}),\n",
    "    \"dqn_policy_1\": (DQNTFPolicy, RockPaperScissors.OBSERVATION_SPACE, RockPaperScissors.ACTION_SPACE, {}),\n",
    "}\n",
    "\n",
    "# Define the PPO trainer\n",
    "ppo_trainer = PPOTrainer(config={\n",
    "    \"env\": RockPaperScissors,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['ppo_policy_1'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"ppo_policy_1\" if agent_id==\"player_1\" else \"dqn_policy_1\",\n",
    "    },\n",
    "    # disable filters, otherwise we would need to synchronize those\n",
    "    # as well to the DQN agent\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "})\n",
    "\n",
    "# Define the DQN trainer\n",
    "dqn_trainer = DQNTrainer(config={\n",
    "    \"env\": RockPaperScissors,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['dqn_policy_1'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"ppo_policy_1\" if agent_id==\"player_1\" else \"dqn_policy_1\",\n",
    "    },\n",
    "})\n",
    "\n",
    "# Alternate training of the two policies\n",
    "stop_reward = 2.9\n",
    "for i in range(20):\n",
    "    print(\"== Iteration\", i, \"==\")\n",
    "\n",
    "    # improve the DQN policy\n",
    "    print(\"-- DQN --\")\n",
    "    result_dqn = dqn_trainer.train()\n",
    "    # print(pretty_print(result_dqn))\n",
    "    print(\"\\tDQN. episode_reward_mean: {}\".format(result_dqn['episode_reward_mean']))\n",
    "\n",
    "    # improve the PPO policy\n",
    "    print(\"-- PPO --\")\n",
    "    result_ppo = ppo_trainer.train()\n",
    "    # print(pretty_print(result_ppo))\n",
    "    print(\"\\tPPO. episode_reward_mean: {}\".format(result_ppo['episode_reward_mean']))\n",
    "\n",
    "    # Test passed gracefully.\n",
    "    if (\n",
    "        result_dqn[\"episode_reward_mean\"] > stop_reward and\n",
    "        result_ppo[\"episode_reward_mean\"] > stop_reward\n",
    "    ):\n",
    "        print(\"test passed (both agents above requested reward)\")\n",
    "        break\n",
    "\n",
    "    # swap weights to synchronize\n",
    "#     dqn_trainer.set_weights(ppo_trainer.get_weights([\"ppo_policy\"]))\n",
    "#     ppo_trainer.set_weights(dqn_trainer.get_weights([\"dqn_policy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the agents\n",
    "Execute in the console:\n",
    "```console\n",
    "tensorboard --logdir=~/ray_results --host=0.0.0.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
