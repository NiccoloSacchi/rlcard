{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Multi Agent Env with Variable-Length Action Spaces in RLlib\n",
    "\n",
    "RLlib on card games:\n",
    "- How to train multiple agents. In particular, every agent (player) should have its own trajectory so that its final reward propagates on his trajectory. Still, all the agents might follow the same policy.\n",
    "    https://docs.ray.io/en/master/rllib-env.html#multi-agent-and-hierarchical \n",
    "- Action space changes depending on the current state. Depending on the cards on the table I might  not be able to play some cards in my hand. In order to mask out some actions:\n",
    "    https://docs.ray.io/en/master/rllib-models.html#variable-length-parametric-action-spaces\n",
    "\n",
    "RLlib can create distinct policies and route agent decisions to its bound policy. When an agent first appears in the env, policy_mapping_fn will be called to determine which policy it is bound to. These assignments are done when the agent first enters the episode, and persist for the duration of the episode.\n",
    "\n",
    "RLlib reports separate training statistics for each policy in the return from train(), along with the combined reward.\n",
    "\n",
    "If all “agents” in the env are homogeneous, then it is possible to use existing single-agent algorithms for training. Since there is still only a single policy being trained, RLlib only needs to internally aggregate the experiences of the different agents prior to policy optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the custom environment\n",
    "https://docs.ray.io/en/latest/rllib-env.html?multi-agent-and-hierarchical#multi-agent-and-hierarchical\n",
    "\n",
    "Example of multi-agent environment:\n",
    "```python\n",
    "# Example: using a multi-agent env\n",
    "> env = MultiAgentTrafficEnv(num_cars=20, num_traffic_lights=5)\n",
    "\n",
    "# Observations are a dict mapping agent names to their obs. Only those\n",
    "# agents' names that require actions in the next call to `step()` will\n",
    "# be present in the returned observation dict.\n",
    "> print(env.reset())\n",
    "{\n",
    "    \"car_1\": [[...]],\n",
    "    \"car_2\": [[...]],\n",
    "    \"traffic_light_1\": [[...]],\n",
    "}\n",
    "\n",
    "# In the following call to `step`, actions should be provided for each\n",
    "# agent that returned an observation before:\n",
    "> new_obs, rewards, dones, infos = env.step(actions={\"car_1\": ..., \"car_2\": ..., \"traffic_light_1\": ...})\n",
    "\n",
    "# Similarly, new_obs, rewards, dones, etc. also become dicts\n",
    "> print(rewards)\n",
    "{\"car_1\": 3, \"car_2\": -1, \"traffic_light_1\": 0}\n",
    "\n",
    "# Individual agents can early exit; The entire episode is done when \"__all__\" = True\n",
    "> print(dones)\n",
    "{\"car_2\": True, \"__all__\": False}\n",
    "```\n",
    "\n",
    "Example of environment with variable-length action spaces. For each agent, also the available actions have to be returned as a mask:\n",
    "```python\n",
    "> print(env.reset())\n",
    "{\n",
    "    \"car_1\": {\n",
    "        \"real_obs\": [[...]],\n",
    "        \"action_mask\": [0, 0, 1, ...]\n",
    "    },\n",
    "    \"car_2\": {\n",
    "        \"real_obs\": [[...]],\n",
    "        \"action_mask\": [1, 0, 1, ...]\n",
    "    },\n",
    "    \"traffic_light_1\": {\n",
    "        \"real_obs\": [[...]],\n",
    "        \"action_mask\": [1, 1, 1, ...]\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Multi Agent Env. game\n",
    "\n",
    "from gym.spaces import Dict, Discrete, Tuple, Box\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.tune.registry import register_env\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Actions:\n",
    "    # number of actions\n",
    "    SIZE = 3\n",
    "\n",
    "    # types o actions\n",
    "    ROCK = 0\n",
    "    PAPER = 1\n",
    "    SCISSORS = 2\n",
    "    NA = 3  # Not Available, hand not yet played\n",
    "\n",
    "class RockPaperScissors(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Two-player environment for the famous rock paper scissors game, modified:\n",
    "    - There are two agents which alternate, the action of one agent provides the\n",
    "        state for the next agent. Since one of the two players begins, the agent\n",
    "        which starts second should learn to always win! The startign player\n",
    "        is drawn randomly.\n",
    "    - The action space changes. The game is divided in three rounds across\n",
    "        which you can't re-use the same action.\n",
    "    \"\"\"\n",
    "\n",
    "    # Action/State spaces\n",
    "    ACTION_SPACE = Discrete(Actions.SIZE)\n",
    "\n",
    "    OBSERVATION_SPACE = Dict({\n",
    "        \"real_obs\": Tuple((\n",
    "            # First round\n",
    "            Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "            # Second round\n",
    "            Tuple((Discrete(4), Discrete(4))),\n",
    "\n",
    "            # Third round\n",
    "            Tuple((Discrete(4), Discrete(4))),\n",
    "        )),\n",
    "\n",
    "        # we have to handle changing action spaces\n",
    "        \"action_mask\": Box(0, 1, shape=(Actions.SIZE, )),\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # Reward mapping\n",
    "    rewards = {\n",
    "        (Actions.ROCK, Actions.ROCK): (0, 0),\n",
    "        (Actions.ROCK, Actions.PAPER): (-1, 1),\n",
    "        (Actions.ROCK, Actions.SCISSORS): (1, -1),\n",
    "        (Actions.PAPER, Actions.ROCK): (1, -1),\n",
    "        (Actions.PAPER, Actions.PAPER): (0, 0),\n",
    "        (Actions.PAPER, Actions.SCISSORS): (-1, 1),\n",
    "        (Actions.SCISSORS, Actions.ROCK): (-1, 1),\n",
    "        (Actions.SCISSORS, Actions.PAPER): (1, -1),\n",
    "        (Actions.SCISSORS, Actions.SCISSORS): (0, 0),\n",
    "    }\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \n",
    "        # state and action spaces\n",
    "        self.action_space = self.ACTION_SPACE\n",
    "        self.observation_space = self.OBSERVATION_SPACE\n",
    "\n",
    "        self.players = [\"player_1\", \"player_2\"]        \n",
    "\n",
    "    def reset(self):\n",
    "        self.player_scores = {p: 0 for p in self.players}  # just used to collect the scores\n",
    "        self.curr_round = 0\n",
    "        self.player_pointer = random.randint(0, 1)\n",
    "        self.state = [\n",
    "            [3, 3],\n",
    "            [3, 3],\n",
    "            [3, 3],\n",
    "        ]\n",
    "\n",
    "        # reward is given to the last player with 1 delay\n",
    "        self.reward_buffer = {p: 0 for p in self.players}\n",
    "        \n",
    "        # actions cannot be reused across one game, we keep a mask for each player\n",
    "        self.action_mask = {p: [1 for _ in range(self.action_space.n)] for p in self.players}\n",
    "\n",
    "        return {self.players[self.player_pointer]: self.get_state(self.players[self.player_pointer])}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # Get current player\n",
    "        curr_player_pointer = self.player_pointer\n",
    "        curr_player = self.players[self.player_pointer]\n",
    "\n",
    "        # Get next player\n",
    "        next_player_pointer = (self.player_pointer + 1) % 2\n",
    "        next_player = self.players[next_player_pointer]\n",
    "    \n",
    "        # Make sure you have the ation only for the current player\n",
    "        assert curr_player in action_dict and len(action_dict) == 1,\\\n",
    "            \"{} should be playing but action {} was received.\".format(curr_player, action_dict)\n",
    "        \n",
    "        # Play the action\n",
    "        curr_action = action_dict[curr_player]\n",
    "        assert self.action_space.contains(curr_action), 'Action {} is not valid'.format(curr_action)\n",
    "        assert self.state[self.curr_round][curr_player_pointer] == Actions.NA,\\\n",
    "            \"Player {} has already played in round {}. Here the current state: {}\".format(\n",
    "            curr_player_pointer,\n",
    "            self.curr_round,\n",
    "            self.state\n",
    "        )        \n",
    "        assert self.action_mask[curr_player][curr_action] == 1, \\\n",
    "            '{} has already played action {}. State: {}'.format(curr_player, curr_action, self.state)\n",
    "        self.action_mask[curr_player][curr_action] = 0  # mask out this action\n",
    "        self.state[self.curr_round][curr_player_pointer] = curr_action\n",
    "\n",
    "        # We might be not done yet\n",
    "        done = {\"__all__\": False}\n",
    "        \n",
    "        # If the next player has already played, the round is done\n",
    "        game_done = False\n",
    "        round_done = self.state[self.curr_round][next_player_pointer] != Actions.NA\n",
    "        if round_done:\n",
    "            # If the round is done we compute the rewards\n",
    "            curr_rewards = self.rewards[tuple(self.state[self.curr_round])]\n",
    "            self.player_scores[\"player_1\"] += curr_rewards[0]\n",
    "            self.player_scores[\"player_2\"] += curr_rewards[1]            \n",
    "            self.reward_buffer[curr_player] = curr_rewards[curr_player_pointer]\n",
    "            \n",
    "            self.curr_round += 1\n",
    "            if self.curr_round == 3:\n",
    "                done = {\"__all__\": True}\n",
    "                # Return reward and state for all players\n",
    "                reward = self.reward_buffer\n",
    "                obs = {p: self.get_state(next_player) for p in self.players}\n",
    "                game_done = True\n",
    "        \n",
    "        # Get the state and reward for the next player\n",
    "        if not game_done:\n",
    "            obs = {next_player: self.get_state(next_player)}\n",
    "            reward = {next_player: self.reward_buffer[next_player]}\n",
    "        \n",
    "        # Move pointer to next player\n",
    "        self.player_pointer = next_player_pointer\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def get_state(self, player):\n",
    "        return {\n",
    "            'real_obs': self.state,\n",
    "            'action_mask': self.action_mask[player]\n",
    "        }\n",
    "    \n",
    "register_env(\"ParametricRPS\", lambda _: RockPaperScissors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_2': {'real_obs': [[3, 3], [3, 3], [3, 3]], 'action_mask': [1, 1, 1]}}\n",
      "\n",
      "Round 0: player_2\n",
      "Insert action (0, 1, 2): 1\n",
      "{'player_1': {'real_obs': [[3, 1], [3, 3], [3, 3]], 'action_mask': [1, 1, 1]}} {'player_1': 0} {'__all__': False}\n",
      "\n",
      "Round 0: player_1\n",
      "Insert action (0, 1, 2): 0\n",
      "{'player_2': {'real_obs': [[0, 1], [3, 3], [3, 3]], 'action_mask': [1, 0, 1]}} {'player_2': 0} {'__all__': False}\n",
      "\n",
      "Round 1: player_2\n",
      "Insert action (0, 1, 2): 0\n",
      "{'player_1': {'real_obs': [[0, 1], [3, 0], [3, 3]], 'action_mask': [0, 1, 1]}} {'player_1': -1} {'__all__': False}\n",
      "\n",
      "Round 1: player_1\n",
      "Insert action (0, 1, 2): 1\n",
      "{'player_2': {'real_obs': [[0, 1], [1, 0], [3, 3]], 'action_mask': [0, 0, 1]}} {'player_2': 0} {'__all__': False}\n",
      "\n",
      "Round 2: player_2\n",
      "Insert action (0, 1, 2): 2\n",
      "{'player_1': {'real_obs': [[0, 1], [1, 0], [3, 2]], 'action_mask': [0, 0, 1]}} {'player_1': 1} {'__all__': False}\n",
      "\n",
      "Round 2: player_1\n",
      "Insert action (0, 1, 2): 2\n",
      "{'player_1': {'real_obs': [[0, 1], [1, 0], [2, 2]], 'action_mask': [0, 0, 0]}, 'player_2': {'real_obs': [[0, 1], [1, 0], [2, 2]], 'action_mask': [0, 0, 0]}} {'player_1': 0, 'player_2': 0} {'__all__': True}\n"
     ]
    }
   ],
   "source": [
    "# Test the environment\n",
    "import random\n",
    "\n",
    "env = RockPaperScissors()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    print('\\nRound {}: {}'.format(env.curr_round, env.players[env.player_pointer]))\n",
    "    action = {list(obs.keys())[0]: int(input('Insert action (0, 1, 2): '))}\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(obs, reward, done)\n",
    "    is_done = done['__all__']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the custom model for Variable-Length Action Spaces\n",
    "https://docs.ray.io/en/master/rllib-models.html#variable-length-parametric-action-spaces\n",
    "Our policy has to take into consideration the fact that some actions might not be executable.\n",
    "\n",
    "See here for which algorithms support parametric actions: https://docs.ray.io/en/master/rllib-algorithms.html#feature-compatibility-matrix\n",
    "\n",
    "\n",
    "**The cartpole example has working configurations for DQN (must set hiddens=[]), PPO (must disable running mean and set vf_share_layers=True), and several other algorithms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.model import flatten\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "def flatten_list(list_of_lists):\n",
    "    flattened = []\n",
    "    for l in list_of_lists:\n",
    "        if isinstance(l, list):\n",
    "            flattened += flatten_list(l)\n",
    "        else:\n",
    "            flattened.append(l)\n",
    "    return flattened\n",
    "\n",
    "class ParametricActionsModel(TFModelV2):\n",
    "    def __init__(self,\n",
    "                 obs_space,\n",
    "                 action_space,\n",
    "                 num_outputs,\n",
    "                 model_config,\n",
    "                 name,\n",
    "                 true_obs_shape=(24,),\n",
    "                 action_embed_size=None):\n",
    "        super(ParametricActionsModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
    "        \n",
    "        if action_embed_size is None:\n",
    "            action_embed_size = action_space.n  # this works for Dicrete() action\n",
    "\n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            obs_space=Box(-1, 1, shape=true_obs_shape),\n",
    "            action_space=action_space,\n",
    "            num_outputs=action_embed_size,\n",
    "            model_config=model_config,\n",
    "            name=name + \"_action_embed\"\n",
    "        )\n",
    "        self.base_model = self.action_embed_model.base_model\n",
    "        self.register_variables(self.action_embed_model.variables())\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):        \n",
    "        # Compute the predicted action probabilties\n",
    "        # input_dict[\"obs\"][\"real_obs\"] is a list of 1d tensors if the observation space is a Tuple while\n",
    "        # it should be a tensor. When it is a list we concatenate the various 1d tensors\n",
    "        obs_concat = input_dict[\"obs\"][\"real_obs\"]\n",
    "        if isinstance(obs_concat, list):\n",
    "            obs_concat = tf.concat(values=flatten_list(obs_concat), axis=1)\n",
    "        action_embed, _ = self.action_embed_model({\"obs\": obs_concat})\n",
    "\n",
    "        # Mask out invalid actions (use tf.float32.min for stability)\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "        return action_embed + inf_mask, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"parametric_model_tf\", ParametricActionsModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-21 19:39:41,237\tINFO resource_spec.py:212 -- Starting Ray with 3.03 GiB memory available for workers and up to 1.52 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-06-21 19:39:41,643\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.125',\n",
       " 'raylet_ip_address': '192.168.1.125',\n",
       " 'redis_address': '192.168.1.125:57511',\n",
       " 'object_store_address': '/tmp/ray/session_2020-06-21_19-39-41_236820_334940/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-06-21_19-39-41_236820_334940/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-06-21_19-39-41_236820_334940'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before training we have to initialize ray\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer_config = {\n",
    "    \"env\": \"ParametricRPS\",  # RockPaperScissors\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"parametric_model_tf\",  # ParametricActionsModel,\n",
    "    },\n",
    "}\n",
    "\n",
    "dqn_trainer_config = {\n",
    "    \"env\": \"ParametricRPS\",  # RockPaperScissors\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"parametric_model_tf\",  # ParametricActionsModel,\n",
    "    },\n",
    "    'hiddens':[],\n",
    "    \"dueling\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Example with tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "stop = {\n",
    "    \"episode_reward_mean\": 2.90,\n",
    "#     \"timesteps_total\": stop_timesteps,\n",
    "#     \"training_iteration\": stop_iters,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/3.03 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/niccolo/ray_results/RLlibExample<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ParametricRPS_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         29.8183</td><td style=\"text-align: right;\">28798</td><td style=\"text-align: right;\"> 2.90987</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = tune.run(\n",
    "    PPOTrainer,\n",
    "    name='RLlibExample',\n",
    "    config=ppo_trainer_config,\n",
    "    verbose=1,\n",
    "    stop=stop\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/3.03 GiB heap, 0.0/1.03 GiB objects<br>Result logdir: /home/niccolo/ray_results/RLlibExample<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_ParametricRPS_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         24.4405</td><td style=\"text-align: right;\">11023</td><td style=\"text-align: right;\"> 2.98204</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = tune.run(\n",
    "    DQNTrainer,\n",
    "    name='RLlibExample',\n",
    "    config=dqn_trainer_config,\n",
    "    verbose=1,\n",
    "    stop=stop\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Plain example without tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-21 19:40:44,257\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-06-21 19:40:44,263\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-06-21 19:40:46,985\tINFO trainable.py:217 -- Getting current IP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=335345)\u001b[0m /home/niccolo/anaconda3/envs/rl/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=335345)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niccolo/anaconda3/envs/rl/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0. episode_reward_mean: 0.030042918454935622\n",
      "Iteration 1. episode_reward_mean: 0.7871064467766117\n",
      "Iteration 2. episode_reward_mean: 1.6071428571428572\n",
      "Iteration 3. episode_reward_mean: 2.1557142857142857\n",
      "Iteration 4. episode_reward_mean: 2.7085714285714286\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(config=ppo_trainer_config)\n",
    "for i in range(5):\n",
    "    res = trainer.train()\n",
    "    print(\"Iteration {}. episode_reward_mean: {}\".format(i, res['episode_reward_mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-21 19:41:09,393\tWARNING trainer_template.py:124 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "2020-06-21 19:41:09,395\tINFO trainable.py:217 -- Getting current IP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0. episode_reward_mean: 0.3413173652694611\n",
      "Iteration 1. episode_reward_mean: 0.3413173652694611\n",
      "Iteration 2. episode_reward_mean: 0.46706586826347307\n",
      "Iteration 3. episode_reward_mean: 0.7904191616766467\n",
      "Iteration 4. episode_reward_mean: 1.221556886227545\n"
     ]
    }
   ],
   "source": [
    "trainer = DQNTrainer(config=dqn_trainer_config)\n",
    "for i in range(5):\n",
    "    res = trainer.train()\n",
    "    print(\"Iteration {}. episode_reward_mean: {}\".format(i, res['episode_reward_mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Example with multiple policies\n",
    "Inspired from: https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_two_trainers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-21 19:41:24,153\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-21 19:41:26,401\tWARNING trainer_template.py:124 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "2020-06-21 19:41:26,404\tINFO trainable.py:217 -- Getting current IP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Iteration 0 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.30538922155688625\n",
      "-- PPO --\n",
      "\u001b[2m\u001b[36m(pid=335354)\u001b[0m /home/niccolo/anaconda3/envs/rl/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=335354)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\tPPO. episode_reward_mean: -0.04504504504504504\n",
      "== Iteration 1 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: -0.03592814371257485\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 0.34234234234234234\n",
      "== Iteration 2 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.25\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 0.781437125748503\n",
      "== Iteration 3 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.4491017964071856\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 0.9864864864864865\n",
      "== Iteration 4 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.5748502994011976\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.3108108108108107\n",
      "== Iteration 5 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 0.9107142857142857\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.3158682634730539\n",
      "== Iteration 6 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.1137724550898203\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.5765765765765767\n",
      "== Iteration 7 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.4550898203592815\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4189189189189189\n",
      "== Iteration 8 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.1785714285714286\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.562874251497006\n",
      "== Iteration 9 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.6706586826347305\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4954954954954955\n",
      "== Iteration 10 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.526946107784431\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.5495495495495495\n",
      "== Iteration 11 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.7142857142857142\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.5\n",
      "== Iteration 12 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.562874251497006\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4864864864864864\n",
      "== Iteration 13 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.473053892215569\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.481981981981982\n",
      "== Iteration 14 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.8214285714285714\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.5044910179640718\n",
      "== Iteration 15 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.598802395209581\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4864864864864864\n",
      "== Iteration 16 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.347305389221557\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.5\n",
      "== Iteration 17 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.375\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.6347305389221556\n",
      "== Iteration 18 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.526946107784431\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.472972972972973\n",
      "== Iteration 19 ==\n",
      "-- DQN --\n",
      "\tDQN. episode_reward_mean: 1.0059880239520957\n",
      "-- PPO --\n",
      "\tPPO. episode_reward_mean: 1.4639639639639639\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "policies = {\n",
    "    \"ppo_policy_1\": (PPOTFPolicy,\n",
    "                     RockPaperScissors.OBSERVATION_SPACE,\n",
    "                     RockPaperScissors.ACTION_SPACE,\n",
    "                     ppo_trainer_config),\n",
    "    \"dqn_policy_1\": (DQNTFPolicy,\n",
    "                     RockPaperScissors.OBSERVATION_SPACE,\n",
    "                     RockPaperScissors.ACTION_SPACE,\n",
    "                     dqn_trainer_config),\n",
    "}\n",
    "\n",
    "# Define the PPO trainer\n",
    "ppo_trainer = PPOTrainer(config={\n",
    "    \"env\": \"ParametricRPS\",  # RockPaperScissors\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['ppo_policy_1'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"ppo_policy_1\" if agent_id==\"player_1\" else \"dqn_policy_1\",\n",
    "    },\n",
    "    # disable filters, otherwise we would need to synchronize those\n",
    "    # as well to the DQN agent\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "})\n",
    "\n",
    "\n",
    "# Define the DQN trainer\n",
    "dqn_trainer = DQNTrainer(config={\n",
    "    \"env\": \"ParametricRPS\",  # RockPaperScissors\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['dqn_policy_1'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"ppo_policy_1\" if agent_id==\"player_1\" else \"dqn_policy_1\",\n",
    "    },\n",
    "})\n",
    "\n",
    "# Alternate training of the two policies\n",
    "stop_reward = 2.9\n",
    "for i in range(20):\n",
    "    print(\"== Iteration\", i, \"==\")\n",
    "\n",
    "    # improve the DQN policy\n",
    "    print(\"-- DQN --\")\n",
    "    result_dqn = dqn_trainer.train()\n",
    "    print(\"\\tDQN. episode_reward_mean: {}\".format(result_dqn['episode_reward_mean']))\n",
    "\n",
    "    # improve the PPO policy\n",
    "    print(\"-- PPO --\")\n",
    "    result_ppo = ppo_trainer.train()\n",
    "    print(\"\\tPPO. episode_reward_mean: {}\".format(result_ppo['episode_reward_mean']))\n",
    "\n",
    "    # Test passed gracefully.\n",
    "    if (\n",
    "        result_dqn[\"episode_reward_mean\"] > stop_reward and\n",
    "        result_ppo[\"episode_reward_mean\"] > stop_reward\n",
    "    ):\n",
    "        print(\"test passed (both agents above requested reward)\")\n",
    "        break\n",
    "\n",
    "    # swap weights to synchronize\n",
    "#     dqn_trainer.set_weights(ppo_trainer.get_weights([\"ppo_policy\"]))\n",
    "#     ppo_trainer.set_weights(dqn_trainer.get_weights([\"dqn_policy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Check Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_activation: relu\n",
      "custom_model: parametric_model_tf\n",
      "custom_options: {}\n",
      "dim: 84\n",
      "fcnet_activation: tanh\n",
      "fcnet_hiddens:\n",
      "- 256\n",
      "- 256\n",
      "framestack: true\n",
      "free_log_std: false\n",
      "grayscale: false\n",
      "lstm_cell_size: 256\n",
      "lstm_use_prev_action_reward: false\n",
      "max_seq_len: 20\n",
      "no_final_linear: false\n",
      "use_lstm: false\n",
      "vf_share_layers: false\n",
      "zero_mean: true\n",
      "\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          6400        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          6400        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 3)            771         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 145,412\n",
      "Trainable params: 145,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "# policy_id = 'default_policy'\n",
    "policy_id = 'ppo_policy_1'\n",
    "# policy_id = 'dqn_policy_1'\n",
    "trainer = dqn_trainer\n",
    "\n",
    "model = trainer.get_policy(policy_id=policy_id).model\n",
    "print(pretty_print(model.model_config))\n",
    "model.base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the agents\n",
    "Execute in the console:\n",
    "```console\n",
    "tensorboard --logdir=~/ray_results --host=0.0.0.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
