{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(game.state.players[0], game.state.players[1], game.state.dealer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rlcard.games.thegame import TheGameGame, TheGamePlayer\n",
    "\n",
    "players = [TheGamePlayer('Nico'), TheGamePlayer('Costy')]\n",
    "game = TheGameGame(\n",
    "    game_size=15,\n",
    "    hand_size=6,\n",
    "    min_cards=2,\n",
    "    players=players,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "game.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rlcard\n",
    "\n",
    "# Make environment and enable human mode\n",
    "env = rlcard.make('leduc-holdem')\n",
    "\n",
    "# Set it to human mode\n",
    "env.set_mode(human_mode=True)\n",
    "\n",
    "print(\">> Leduc Hold'em pre-trained model\")\n",
    "\n",
    "# Reset environment\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = input('>> You choose action (integer): ')\n",
    "    while not action.isdigit() or int(action) not in state['legal_actions']:\n",
    "        print('Action illegel...')\n",
    "        action = input('>> Re-choose action (integer): ')\n",
    "         \n",
    "    state, _, _ = env.step(int(action))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Niccolo\\Anaconda3\\envs\\rl\\lib\\site-packages\\sonnet\\python\\modules\\util.py:63: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rlcard\n",
    "from rlcard.agents.random_agent import RandomAgent\n",
    "\n",
    "# Make environment\n",
    "env = rlcard.make('limit-holdem')\n",
    "\n",
    "agent = RandomAgent(action_num=env.action_num)\n",
    "\n",
    "env.set_agents([agent, agent, agent])\n",
    "\n",
    "trajectories, payoffs = env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 0\n",
      "State: {'obs': array([21,  3]), 'legal_actions': [0, 1]}, Action: 0, Reward: 0, Next State: {'obs': array([15,  3]), 'legal_actions': [0, 1]}, Done: False\n",
      "State: {'obs': array([15,  3]), 'legal_actions': [0, 1]}, Action: 1, Reward: -1, Next State: {'obs': array([15, 20]), 'legal_actions': [0, 1]}, Done: True\n",
      "\n",
      "Episode 1\n",
      "State: {'obs': array([15,  5]), 'legal_actions': [0, 1]}, Action: 1, Reward: 1, Next State: {'obs': array([15, 23]), 'legal_actions': [0, 1]}, Done: True\n"
     ]
    }
   ],
   "source": [
    "import rlcard\n",
    "from rlcard.agents.random_agent import RandomAgent\n",
    "from rlcard.utils.utils import set_global_seed\n",
    "\n",
    "# Make environment\n",
    "env = rlcard.make('blackjack')\n",
    "episode_num = 2\n",
    "\n",
    "# Set a global seed\n",
    "set_global_seed(0)\n",
    "\n",
    "# Set up agents\n",
    "agent_0 = RandomAgent(action_num=env.action_num)\n",
    "env.set_agents([agent_0])\n",
    "\n",
    "for episode in range(episode_num):\n",
    "\n",
    "    # Generate data from the environment\n",
    "    trajectories, _ = env.run(is_training=False)\n",
    "\n",
    "    # Print out the trajectories\n",
    "    print('\\nEpisode {}'.format(episode))\n",
    "    for ts in trajectories[0]:\n",
    "        print('State: {}, Action: {}, Reward: {}, Next State: {}, Done: {}'.format(ts[0], ts[1], ts[2], ts[3], ts[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\niccolo\\documents\\courses\\reinforcement learning\\rlcard\\rlcard\\utils\\utils.py:328: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'train_every'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-94fa1e2cb700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m                      \u001b[0mtrain_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_every\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                      \u001b[0mstate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                      mlp_layers=[128,128])\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;31m# Initialize global variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'train_every'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents.dqn_agent import DQNAgent\n",
    "from rlcard.agents.random_agent import RandomAgent\n",
    "from rlcard.utils.utils import set_global_seed\n",
    "from rlcard.utils.logger import Logger\n",
    "\n",
    "# Make environment\n",
    "env = rlcard.make('leduc-holdem')\n",
    "eval_env = rlcard.make('leduc-holdem')\n",
    "\n",
    "# Set the iterations numbers and how frequently we evaluate/save plot\n",
    "evaluate_every = 1000\n",
    "evaluate_num = 10000\n",
    "timesteps = 100000\n",
    "\n",
    "# The intial memory size\n",
    "memory_init_size = 1000\n",
    "\n",
    "# Train the agent every X steps\n",
    "train_every = 1\n",
    "\n",
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/leduc_holdem_single_dqn_result/'\n",
    "\n",
    "# Set a global seed\n",
    "set_global_seed(0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize a global step\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Set up the agents\n",
    "    agent = DQNAgent(sess,\n",
    "                     scope='dqn',\n",
    "                     action_num=env.action_num,\n",
    "                     replay_memory_init_size=memory_init_size,\n",
    "                     train_every=train_every,\n",
    "                     state_shape=env.state_shape,\n",
    "                     mlp_layers=[128,128])\n",
    "    # Initialize global variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Init a Logger to plot the learning curve\n",
    "    logger = Logger(log_dir)\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for timestep in range(timesteps):\n",
    "        action = agent.step(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        ts = (state, action, reward, next_state, done)\n",
    "        agent.feed(ts)\n",
    "\n",
    "        if timestep % evaluate_every == 0:\n",
    "            rewards = []\n",
    "            state = eval_env.reset()\n",
    "            for _ in range(evaluate_num):\n",
    "                action, _ = agent.eval_step(state)\n",
    "                _, reward, done = env.step(action)\n",
    "                if done:\n",
    "                    rewards.append(reward)\n",
    "            logger.log_performance(env.timestep, np.mean(rewards))\n",
    "\n",
    "    # Close files in the logger\n",
    "    logger.close_files()\n",
    "\n",
    "    # Plot the learning curve\n",
    "    logger.plot('DQN')\n",
    "    \n",
    "    # Save model\n",
    "    save_dir = 'models/leduc_holdem_single_dqn'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, os.path.join(save_dir, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Niccolo\\Anaconda3\\envs\\rl\\lib\\site-packages\\sonnet\\python\\modules\\util.py:63: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rlcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "n = 10000\n",
    "a = torch.rand(n, n)\n",
    "b = torch.rand(n, n)\n",
    "c = a.cuda()\n",
    "d = b.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2497.8679, 2508.6377, 2523.4033,  ..., 2469.9338, 2528.7549,\n",
       "         2518.1609],\n",
       "        [2459.0049, 2499.8340, 2527.2683,  ..., 2461.7351, 2518.4272,\n",
       "         2501.4077],\n",
       "        [2506.0278, 2526.7751, 2532.4163,  ..., 2486.4153, 2547.0908,\n",
       "         2524.3271],\n",
       "        ...,\n",
       "        [2501.7178, 2528.1509, 2532.3638,  ..., 2468.3743, 2540.3877,\n",
       "         2518.7964],\n",
       "        [2477.8696, 2494.7334, 2508.3547,  ..., 2473.5212, 2541.2661,\n",
       "         2511.5283],\n",
       "        [2478.0259, 2506.8237, 2530.0471,  ..., 2483.1580, 2528.5720,\n",
       "         2521.1538]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2497.8667, 2508.6411, 2523.4062,  ..., 2469.9275, 2528.7561,\n",
       "         2518.1594],\n",
       "        [2459.0076, 2499.8340, 2527.2708,  ..., 2461.7388, 2518.4268,\n",
       "         2501.4011],\n",
       "        [2506.0288, 2526.7732, 2532.4124,  ..., 2486.4226, 2547.0859,\n",
       "         2524.3232],\n",
       "        ...,\n",
       "        [2501.7112, 2528.1499, 2532.3699,  ..., 2468.3735, 2540.3860,\n",
       "         2518.7883],\n",
       "        [2477.8711, 2494.7378, 2508.3528,  ..., 2473.5266, 2541.2656,\n",
       "         2511.5308],\n",
       "        [2478.0205, 2506.8279, 2530.0410,  ..., 2483.1538, 2528.5693,\n",
       "         2521.1553]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time c@d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation (random policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 4\n",
    "rewards = -1 * np.ones((4, 4))\n",
    "rewards[0, 0] = rewards[-1, -1] = 0\n",
    "\n",
    "values = np.zeros((4, 4))\n",
    "\n",
    "def expected_next_value(values, curr_state_x, curr_state_y):\n",
    "    exp_value = 0\n",
    "    if (curr_state_x==0 and curr_state_y==0) or (curr_state_x==n-1 and curr_state_y==n-1):\n",
    "        return 0\n",
    "    for i in [-1, 1]:\n",
    "        new_state_x = np.clip(curr_state_x + i, 0, n-1)\n",
    "        new_state_y = np.clip(curr_state_y + i, 0, n-1)\n",
    "        # we end in new_state with 1/4 prob\n",
    "        exp_value += 1/4*values[curr_state_x, new_state_y]\n",
    "        exp_value += 1/4*values[new_state_x, curr_state_y]\n",
    "    return exp_value\n",
    "\n",
    "for x in range(500):\n",
    "    new_values = np.zeros((4, 4))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            new_values[i, j] = rewards[i, j] + expected_next_value(values, i, j)\n",
    "    values = new_values\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
