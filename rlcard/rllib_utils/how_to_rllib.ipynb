{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from rlcard.rllib_utils.random_policy import RandomPolicy\n",
    "from rlcard.rllib_utils.model import ParametricActionsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from rlcard.rllib_utils.rlcard_wrapper import RLCardWrapper\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which RLcard environment to use\n",
    "# rlcard_env_id = 'blackjack'\n",
    "# rlcard_env_id = 'doudizhu'\n",
    "# rlcard_env_id = 'gin-rummy'\n",
    "rlcard_env_id = 'leduc-holdem'\n",
    "# rlcard_env_id = 'limit-holdem'\n",
    "# rlcard_env_id = 'mahjong'\n",
    "# rlcard_env_id = 'no-limit-holdem'\n",
    "# rlcard_env_id = 'simple-doudizhu'\n",
    "# rlcard_env_id = 'uno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"rlcard_env_id\": rlcard_env_id,\n",
    "    \"randomize_agents_eval\": [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e7988b6f74b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_cpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\worker.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(address, redis_address, redis_port, num_cpus, num_gpus, memory, object_store_memory, resources, driver_object_store_memory, redis_max_memory, log_to_driver, node_ip_address, object_id_seed, local_mode, redirect_worker_output, redirect_output, ignore_reinit_error, num_redis_shards, redis_max_clients, redis_password, plasma_directory, huge_pages, include_java, include_webui, webui_host, job_id, configure_logging, logging_level, logging_format, plasma_store_socket_name, raylet_socket_name, temp_dir, load_code_from_local, java_worker_options, use_pickle, _internal_config, lru_evict)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m             raise RuntimeError(\"Maybe you called ray.init twice by accident? \"\n\u001b[0m\u001b[0;32m    660\u001b[0m                                \u001b[1;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m                                \u001b[1;34m\"'ignore_reinit_error=True' or by calling \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register env and model to be used by rllib\n",
    "rlcard_environment = lambda _: RLCardWrapper(env_config)\n",
    "register_env(rlcard_env_id, rlcard_environment)\n",
    "ModelCatalog.register_custom_model(\"parametric_model_tf\", ParametricActionsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tmp = rlcard_environment(None)\n",
    "policies = {\n",
    "    \"ppo_policy_1\": (PPOTFPolicy,\n",
    "                     env_tmp.observation_space,\n",
    "                     env_tmp.action_space,\n",
    "                     ppo_trainer_config),\n",
    "    \"rand_policy\": (RandomPolicy,\n",
    "                    env_tmp.observation_space,\n",
    "                    env_tmp.action_space,\n",
    "                    {}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 00:29:01,570\tERROR syncer.py:46 -- Log sync requires rsync to be installed.\n",
      "2020-11-17 00:29:01,586\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "2020-11-17 00:29:04,324\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-11-17 00:29:04,349\tERROR syncer.py:46 -- Log sync requires rsync to be installed.\n",
      "2020-11-17 00:29:04,364\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "2020-11-17 00:29:07,785\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Define the trainer\n",
    "ppo_trainer_config = {\n",
    "    # \"env\": rlcard_env_id,\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"parametric_model_tf\",\n",
    "    },\n",
    "}\n",
    "\n",
    "trainer = PPOTrainer(config={\n",
    "    \"env\": rlcard_env_id,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['ppo_policy_1'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"ppo_policy_1\",\n",
    "    },\n",
    "    # \"num_gpus\": 0.5,\n",
    "    # \"num_gpus_per_worker\": 0,\n",
    "})\n",
    "\n",
    "trainer_eval = PPOTrainer(config={\n",
    "    \"env\": rlcard_env_id,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['ppo_policy_1'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"ppo_policy_1\" if agent_id == \"player_1\" else \"rand_policy\",\n",
    "    },\n",
    "    # \"num_gpus\": 0.5,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0. policy_reward_mean: ['ppo_policy_1: 1.21875', 'rand_policy: -1.21875']\n",
      "Iteration 1. policy_reward_mean: ['ppo_policy_1: 1.1794171220400729', 'rand_policy: -1.1794171220400729']\n",
      "Iteration 2. policy_reward_mean: ['ppo_policy_1: 1.2281938325991189', 'rand_policy: -1.2281938325991189']\n",
      "Iteration 3. policy_reward_mean: ['ppo_policy_1: 1.2985803016858917', 'rand_policy: -1.2985803016858917']\n",
      "Iteration 4. policy_reward_mean: ['ppo_policy_1: 1.269642857142857', 'rand_policy: -1.269642857142857']\n",
      "Iteration 5. policy_reward_mean: ['ppo_policy_1: 1.2637867647058822', 'rand_policy: -1.2637867647058822']\n",
      "Iteration 6. policy_reward_mean: ['ppo_policy_1: 1.400699912510936', 'rand_policy: -1.400699912510936']\n",
      "Iteration 7. policy_reward_mean: ['ppo_policy_1: 1.2151060070671378', 'rand_policy: -1.2151060070671378']\n",
      "Iteration 8. policy_reward_mean: ['ppo_policy_1: 1.3214928057553956', 'rand_policy: -1.3214928057553956']\n",
      "Iteration 9. policy_reward_mean: ['ppo_policy_1: 1.1961400359066428', 'rand_policy: -1.1961400359066428']\n",
      "Iteration 10. policy_reward_mean: ['ppo_policy_1: 1.2437163375224416', 'rand_policy: -1.2437163375224416']\n",
      "Iteration 11. policy_reward_mean: ['ppo_policy_1: 1.2313829787234043', 'rand_policy: -1.2313829787234043']\n",
      "Iteration 12. policy_reward_mean: ['ppo_policy_1: 1.333180987202925', 'rand_policy: -1.333180987202925']\n",
      "Iteration 13. policy_reward_mean: ['ppo_policy_1: 1.1956709956709957', 'rand_policy: -1.1956709956709957']\n",
      "Iteration 14. policy_reward_mean: ['ppo_policy_1: 1.381918819188192', 'rand_policy: -1.381918819188192']\n",
      "Iteration 15. policy_reward_mean: ['ppo_policy_1: 1.2166813768755516', 'rand_policy: -1.2166813768755516']\n",
      "Iteration 16. policy_reward_mean: ['ppo_policy_1: 1.3972972972972972', 'rand_policy: -1.3972972972972972']\n",
      "Iteration 17. policy_reward_mean: ['ppo_policy_1: 1.220496894409938', 'rand_policy: -1.220496894409938']\n",
      "Iteration 18. policy_reward_mean: ['ppo_policy_1: 1.4081177520071364', 'rand_policy: -1.4081177520071364']\n",
      "Iteration 19. policy_reward_mean: ['ppo_policy_1: 1.273851590106007', 'rand_policy: -1.273851590106007']\n",
      "Training finished (00:03:41), check the results in ~/ray_results/<dir>/\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i in range(20):\n",
    "    trainer.train()\n",
    "\n",
    "    trainer_eval.set_weights(trainer.get_weights([\"ppo_policy_1\"]))\n",
    "    res = trainer_eval.train()\n",
    "\n",
    "    policy_rewards = sorted(['{}: {}'.format(k, v) for k, v in res['policy_reward_mean'].items()])\n",
    "    print(\"Iteration {}. policy_reward_mean: {}\".format(i, policy_rewards))\n",
    "\n",
    "stop = time.time()\n",
    "train_duration = time.strftime('%H:%M:%S', time.gmtime(stop-start))\n",
    "print('Training finished ({}), check the results in ~/ray_results/<dir>/'.format(train_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
