{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning card games with RLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from rlcard.rllib_utils.random_policy import RandomPolicy\n",
    "from rlcard.rllib_utils.model import ParametricActionsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from rlcard.rllib_utils.rlcard_wrapper import RLCardWrapper\n",
    "from rlcard.rllib_utils.custom_metrics import PlayerScoreCallbacks\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLCard environments\n",
    "\n",
    "RLCard is a Python library implementing some of the most popular card games, including Blackjack and some different flavours of Poker. In our fork of this library you can also experiment with a popular Italian game, *Scopone*.\n",
    "Card games are a nice playgroiund for Reinforcement Learning because the reward is often straightforward, while the size of the state space can become huge quite fast, depending on the game. Furthermore, besides single player games like Blackjack, there are many multiplayer games in which the agents have to compete or collaborate.\n",
    "We implemented a wrapper of RLLib for RLCard environments, which allows the researcher a lot of flexibility in assigning the same policy or different ones to each agent, experimenting different techniques of solving Multi-Agent Reinforcement Learning (MARL) problems.\n",
    "For example, in a game like *Scopone*, in which 2 pairs of players play against each other, one can try to assign the same policy to each player, 2 different policies (one per team or one per player of the team) or 4 different policies, while also deciding whether to train all of them together or to freeze some of them while training the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which RLcard environment to use\n",
    "# rlcard_env_id = 'blackjack'\n",
    "# rlcard_env_id = 'doudizhu'\n",
    "# rlcard_env_id = 'gin-rummy'\n",
    "# rlcard_env_id = 'leduc-holdem'\n",
    "# rlcard_env_id = 'limit-holdem'\n",
    "# rlcard_env_id = 'mahjong'\n",
    "# rlcard_env_id = 'no-limit-holdem'\n",
    "# rlcard_env_id = 'simple-doudizhu'\n",
    "# rlcard_env_id = 'uno'\n",
    "rlcard_env_id = 'scopone'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the environment, the policies, the trainer\n",
    "\n",
    "Here we prepare the configuration of the training and the evaluation environment. The only difference is that we set some agents to behave randomly during the evaluation, so that we can estimate the increase in performance during the training. A random agent is most likely not a good benchmark, but we do not have any better deterministic baseline. One might want to use a pre-trained agent as a baseline, but we have not implemented this feature in this notebook. Please see *policy_arena.py* to have an idea how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"rlcard_env_id\": rlcard_env_id,\n",
    "}\n",
    "\n",
    "env_config_eval = {\n",
    "    \"rlcard_env_id\": rlcard_env_id,\n",
    "    \"explore\": False\n",
    "#     \"randomize_agents_eval\": ['player_2', 'player_4']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 08:41:30,714\tINFO resource_spec.py:212 -- Starting Ray with 4.3 GiB memory available for workers and up to 2.17 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-11-19 08:41:31,593\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.92.120.113',\n",
       " 'raylet_ip_address': '10.92.120.113',\n",
       " 'redis_address': '10.92.120.113:6379',\n",
       " 'object_store_address': 'tcp://127.0.0.1:62370',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:62237',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': 'C:\\\\Users\\\\chiappal\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2020-11-19_08-41-30_707813_21380'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 08:41:34,035\tWARNING worker.py:1047 -- The dashboard on node CRDWCL01169 failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\dashboard/dashboard.py\", line 960, in <module>\n",
      "    metrics_export_address=metrics_export_address)\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\dashboard/dashboard.py\", line 513, in __init__\n",
      "    build_dir = setup_static_dir(self.app)\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\dashboard/dashboard.py\", line 414, in setup_static_dir\n",
      "    \"&& npm run build)\", build_dir)\n",
      "FileNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm ci && npm run build): 'c:\\\\users\\\\chiappal\\\\appdata\\\\local\\\\continuum\\\\miniconda3\\\\envs\\\\rl\\\\lib\\\\site-packages\\\\ray\\\\dashboard\\\\client/build'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register env and model to be used by rllib\n",
    "rlcard_environment = lambda _: RLCardWrapper(env_config)\n",
    "register_env(rlcard_env_id, rlcard_environment)\n",
    "ModelCatalog.register_custom_model(\"parametric_model_tf\", ParametricActionsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tmp = rlcard_environment(None)\n",
    "policy_class = PPOTFPolicy\n",
    "policy_config = {\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"parametric_model_tf\",\n",
    "        \"fcnet_hiddens\": [256, 256],\n",
    "        \"fcnet_activation\": \"relu\"\n",
    "    },\n",
    "}\n",
    "\n",
    "policies = {\n",
    "    \"ppo_policy_1\": (policy_class,\n",
    "                     env_tmp.observation_space,\n",
    "                     env_tmp.action_space,\n",
    "                     policy_config),\n",
    "    \"ppo_policy_2\": (policy_class,\n",
    "                     env_tmp.observation_space,\n",
    "                     env_tmp.action_space,\n",
    "                     policy_config),\n",
    "    \"rand_policy\": (RandomPolicy,\n",
    "                    env_tmp.observation_space,\n",
    "                    env_tmp.action_space,\n",
    "                    {}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_class = PPOTrainer\n",
    "\n",
    "agent_to_policy_dict = {\n",
    "    \"player_1\": \"ppo_policy_1\",\n",
    "    \"player_2\": \"ppo_policy_2\",\n",
    "    \"player_3\": \"ppo_policy_1\",\n",
    "    \"player_4\": \"ppo_policy_2\"\n",
    "}\n",
    "\n",
    "trainer_config = {\n",
    "    \"env\": rlcard_env_id,\n",
    "    \"env_config\": env_config,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['ppo_policy_1'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: agent_to_policy_dict[agent_id],\n",
    "    },\n",
    "    \"timesteps_per_iteration\": 10000,\n",
    "    \"num_workers\": 3,\n",
    "    \"evaluation_num_workers\": 0,\n",
    "    \"evaluation_config\": {\n",
    "        \"env_config\": env_config_eval\n",
    "    },\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"evaluation_interval\": 1,\n",
    "    \"callbacks\": PlayerScoreCallbacks\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "trainer = trainer_class(trainer_config)\n",
    "for i in range(20):\n",
    "    res = trainer.train()\n",
    "\n",
    "#     trainer_eval.set_weights(trainer.get_weights([\"ppo_policy_1\"]))\n",
    "#     res = trainer_eval.train()\n",
    "\n",
    "    policy_rewards = sorted(['{}: {}'.format(k, v) for k, v in res['policy_reward_mean'].items()])\n",
    "    print(\"Iteration {}. policy_reward_mean: {}\".format(i, policy_rewards))\n",
    "\n",
    "stop = time.time()\n",
    "train_duration = time.strftime('%H:%M:%S', time.gmtime(stop-start))\n",
    "print('Training finished ({}), check the results in ~/ray_results/<dir>/'.format(train_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 08:41:58,005\tERROR syncer.py:46 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m 2020-11-19 08:42:02,158\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m 2020-11-19 08:42:02,158\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m 2020-11-19 08:42:02,180\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m 2020-11-19 08:42:03,178\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m 2020-11-19 08:42:08,236\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m 2020-11-19 08:42:09,802\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=13712)\u001b[0m 2020-11-19 08:42:10,139\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18700)\u001b[0m 2020-11-19 08:42:10,190\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=16544)\u001b[0m 2020-11-19 08:42:10,250\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=13712)\u001b[0m 2020-11-19 08:42:12,069\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m 2020-11-19 08:42:12,132\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=18700)\u001b[0m 2020-11-19 08:42:12,153\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=16544)\u001b[0m 2020-11-19 08:42:12,203\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=16544)\u001b[0m c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\numpy\\core\\_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=16544)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\numpy\\core\\_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=18672)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-19 08:42:42,118\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n",
      "2020-11-19 08:43:06,534\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n",
      "2020-11-19 08:43:30,351\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n",
      "2020-11-19 08:43:51,560\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n",
      "2020-11-19 08:44:12,431\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n",
      "2020-11-19 08:44:33,292\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n",
      "2020-11-19 08:44:56,769\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n",
      "2020-11-19 08:45:18,340\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18-scopone\\\\experiment_state-2020-11-19_08-41-57.json'\n"
     ]
    }
   ],
   "source": [
    "res = tune.run(\n",
    "    trainer_class,\n",
    "    name=\"2020-11-18-scopone\",  # This is used to specify the logging directory.\n",
    "    stop={\n",
    "        \"training_iteration\": 1000,\n",
    "#         \"episodes_total\": 10000\n",
    "    },\n",
    "    verbose=0,\n",
    "    config=trainer_config,\n",
    "    local_dir=\"./outputs\",\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True,\n",
    "    restore=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize tensorboard: tensorboard --logdir=./outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
