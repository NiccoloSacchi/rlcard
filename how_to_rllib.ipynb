{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from rlcard.rllib_utils.random_policy import RandomPolicy\n",
    "from rlcard.rllib_utils.model import ParametricActionsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from rlcard.rllib_utils.rlcard_wrapper import RLCardWrapper\n",
    "from rlcard.rllib_utils.custom_metrics import PlayerScoreCallbacks\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which RLcard environment to use\n",
    "# rlcard_env_id = 'blackjack'\n",
    "# rlcard_env_id = 'doudizhu'\n",
    "# rlcard_env_id = 'gin-rummy'\n",
    "rlcard_env_id = 'leduc-holdem'\n",
    "# rlcard_env_id = 'limit-holdem'\n",
    "# rlcard_env_id = 'mahjong'\n",
    "# rlcard_env_id = 'no-limit-holdem'\n",
    "# rlcard_env_id = 'simple-doudizhu'\n",
    "# rlcard_env_id = 'uno'\n",
    "# rlcard_env_id = 'scopone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"rlcard_env_id\": rlcard_env_id,\n",
    "}\n",
    "\n",
    "env_config_eval = {\n",
    "    \"rlcard_env_id\": rlcard_env_id,\n",
    "    \"randomize_agents_eval\": [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 07:59:09,704\tINFO resource_spec.py:212 -- Starting Ray with 4.0 GiB memory available for workers and up to 2.02 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-11-18 07:59:11,318\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.92.120.113',\n",
       " 'raylet_ip_address': '10.92.120.113',\n",
       " 'redis_address': '10.92.120.113:6379',\n",
       " 'object_store_address': 'tcp://127.0.0.1:65144',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:64663',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': 'C:\\\\Users\\\\chiappal\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2020-11-18_07-59-09_672995_3588'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register env and model to be used by rllib\n",
    "rlcard_environment = lambda _: RLCardWrapper(env_config)\n",
    "register_env(rlcard_env_id, rlcard_environment)\n",
    "ModelCatalog.register_custom_model(\"parametric_model_tf\", ParametricActionsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tmp = rlcard_environment(None)\n",
    "policy_class = PPOTFPolicy\n",
    "policy_config = {\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"parametric_model_tf\",\n",
    "        \"fcnet_hiddens\": [256, 256],\n",
    "        \"fcnet_activation\": \"relu\"\n",
    "    },\n",
    "}\n",
    "\n",
    "policies = {\n",
    "    \"ppo_policy_1\": (policy_class,\n",
    "                     env_tmp.observation_space,\n",
    "                     env_tmp.action_space,\n",
    "                     policy_config),\n",
    "    \"ppo_policy_2\": (policy_class,\n",
    "                     env_tmp.observation_space,\n",
    "                     env_tmp.action_space,\n",
    "                     policy_config),\n",
    "    \"rand_policy\": (RandomPolicy,\n",
    "                    env_tmp.observation_space,\n",
    "                    env_tmp.action_space,\n",
    "                    {}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_class = PPOTrainer\n",
    "trainer_config = {\n",
    "    \"env\": rlcard_env_id,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": ['ppo_policy_1', 'ppo_policy_2'],\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"ppo_policy_1\" if agent_id == \"player_1\" else \"ppo_policy_2\",\n",
    "    },\n",
    "    \"num_workers\": 2,\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \"env_config\": env_config_eval\n",
    "    },\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"evaluation_interval\": 2,\n",
    "    \"callbacks\": PlayerScoreCallbacks\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "trainer = trainer_class(trainer_config)\n",
    "for i in range(20):\n",
    "    res = trainer.train()\n",
    "\n",
    "#     trainer_eval.set_weights(trainer.get_weights([\"ppo_policy_1\"]))\n",
    "#     res = trainer_eval.train()\n",
    "\n",
    "    policy_rewards = sorted(['{}: {}'.format(k, v) for k, v in res['policy_reward_mean'].items()])\n",
    "    print(\"Iteration {}. policy_reward_mean: {}\".format(i, policy_rewards))\n",
    "\n",
    "stop = time.time()\n",
    "train_duration = time.strftime('%H:%M:%S', time.gmtime(stop-start))\n",
    "print('Training finished ({}), check the results in ~/ray_results/<dir>/'.format(train_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:23:15,274\tERROR syncer.py:46 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:23:38,487\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:23:38,487\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:23:38,557\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:23:43,822\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=21164)\u001b[0m 2020-11-18 08:24:20,718\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=14476)\u001b[0m 2020-11-18 08:24:20,716\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:24:22,797\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=21164)\u001b[0m 2020-11-18 08:24:27,593\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=14476)\u001b[0m 2020-11-18 08:24:27,557\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:24:29,294\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:24:37,351\tINFO trainable.py:181 -- _setup took 58.876 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m 2020-11-18 08:24:37,352\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=21164)\u001b[0m c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\numpy\\core\\_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=21164)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\numpy\\core\\_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=18668)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "\u001b[2m\u001b[36m(pid=12192)\u001b[0m 2020-11-18 08:25:10,730\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=12192)\u001b[0m 2020-11-18 08:25:18,874\tWARNING deprecation.py:30 -- DeprecationWarning: `ray.rllib.models.tf.fcnet_v2.FullyConnectedNetwork` has been deprecated. Use `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` instead. This will raise an error in the future!\n",
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: -0.03236040609137056\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: 0.03236040609137056\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 2.5418781725888326\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1576\n",
      "  episodes_total: 1576\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0206798315048218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05411197245121002\n",
      "        model: {}\n",
      "        policy_loss: -0.07758011668920517\n",
      "        total_loss: 4.853428363800049\n",
      "        vf_explained_var: 0.0487494021654129\n",
      "        vf_loss: 4.920185565948486\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0186395645141602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05402515456080437\n",
      "        model: {}\n",
      "        policy_loss: -0.0860595703125\n",
      "        total_loss: 4.99539041519165\n",
      "        vf_explained_var: 0.037489138543605804\n",
      "        vf_loss: 5.070645332336426\n",
      "    num_steps_sampled: 4006\n",
      "    num_steps_trained: 4006\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.04067796610168\n",
      "    ram_util_percent: 66.79661016949152\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: -0.03236040609137056\n",
      "    ppo_policy_2: 0.03236040609137056\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.5480048714065082\n",
      "    mean_inference_ms: 5.218291115942945\n",
      "    mean_processing_ms: 3.3447472066557116\n",
      "  time_since_restore: 41.04771637916565\n",
      "  time_this_iter_s: 41.04771637916565\n",
      "  time_total_s: 41.04771637916565\n",
      "  timers:\n",
      "    learn_throughput: 251.063\n",
      "    learn_time_ms: 15956.18\n",
      "    load_throughput: 2339.202\n",
      "    load_time_ms: 1712.55\n",
      "    sample_throughput: 213.423\n",
      "    sample_time_ms: 18770.215\n",
      "    update_time_ms: 29.077\n",
      "  timestamp: 1605684318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4006\n",
      "  training_iteration: 1\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:25:19,761\tWARNING util.py:137 -- The `process_trial` operation took 0.6582911014556885 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:25:19,893\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         41.0477</td><td style=\"text-align: right;\">4006</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=12192)\u001b[0m c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\numpy\\core\\_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(pid=12192)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: 0.03080168776371308\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: -0.03080168776371308\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 3.3755274261603376\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1185\n",
      "  episodes_total: 2761\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      player_1_score_max: 7.0\n",
      "      player_1_score_mean: 0.12\n",
      "      player_1_score_min: -7.0\n",
      "      player_2_score_max: 7.0\n",
      "      player_2_score_mean: -0.12\n",
      "      player_2_score_min: -7.0\n",
      "    episode_len_mean: 3.95\n",
      "    episode_reward_max: 0.0\n",
      "    episode_reward_mean: 0.0\n",
      "    episode_reward_min: 0.0\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 5\n",
      "      - 6\n",
      "      - 2\n",
      "      - 3\n",
      "      - 4\n",
      "      - 1\n",
      "      - 4\n",
      "      - 5\n",
      "      - 4\n",
      "      - 5\n",
      "      - 2\n",
      "      - 4\n",
      "      - 2\n",
      "      - 6\n",
      "      - 5\n",
      "      - 2\n",
      "      - 4\n",
      "      - 6\n",
      "      - 1\n",
      "      - 5\n",
      "      - 4\n",
      "      - 5\n",
      "      - 1\n",
      "      - 8\n",
      "      - 4\n",
      "      - 3\n",
      "      - 6\n",
      "      - 3\n",
      "      - 6\n",
      "      - 2\n",
      "      - 4\n",
      "      - 2\n",
      "      - 6\n",
      "      - 7\n",
      "      - 5\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 4\n",
      "      - 5\n",
      "      - 3\n",
      "      - 5\n",
      "      - 3\n",
      "      - 5\n",
      "      - 3\n",
      "      - 1\n",
      "      - 5\n",
      "      - 7\n",
      "      - 7\n",
      "      - 6\n",
      "      - 5\n",
      "      - 5\n",
      "      - 6\n",
      "      - 1\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 5\n",
      "      - 4\n",
      "      - 6\n",
      "      - 2\n",
      "      - 1\n",
      "      - 5\n",
      "      - 4\n",
      "      - 4\n",
      "      - 5\n",
      "      - 2\n",
      "      - 4\n",
      "      - 4\n",
      "      - 3\n",
      "      - 1\n",
      "      - 5\n",
      "      - 3\n",
      "      - 4\n",
      "      - 5\n",
      "      - 1\n",
      "      - 6\n",
      "      - 6\n",
      "      - 1\n",
      "      - 6\n",
      "      - 2\n",
      "      - 5\n",
      "      - 2\n",
      "      - 4\n",
      "      - 3\n",
      "      - 1\n",
      "      - 4\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 4\n",
      "      - 3\n",
      "      - 4\n",
      "      - 5\n",
      "      - 3\n",
      "      - 3\n",
      "      - 5\n",
      "      - 3\n",
      "      - 1\n",
      "      - 1\n",
      "      episode_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      policy_ppo_policy_1_reward:\n",
      "      - -4.0\n",
      "      - -7.0\n",
      "      - 1.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 6.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 3.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - -4.0\n",
      "      - 1.0\n",
      "      - -2.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 7.0\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 4.0\n",
      "      - 2.0\n",
      "      - -4.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - 6.0\n",
      "      - -4.0\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - -2.0\n",
      "      - -2.0\n",
      "      - 5.0\n",
      "      - -1.0\n",
      "      - 6.0\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - -5.0\n",
      "      - 3.0\n",
      "      - 4.0\n",
      "      - -5.0\n",
      "      - -1.0\n",
      "      - -4.0\n",
      "      - 1.0\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - 2.0\n",
      "      - -1.0\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - 5.0\n",
      "      - -3.0\n",
      "      - 0.5\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      - 3.0\n",
      "      - -1.0\n",
      "      - -2.0\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - 2.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 3.0\n",
      "      - -1.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      policy_ppo_policy_2_reward:\n",
      "      - 4.0\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      - 2.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -6.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - -3.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 2.0\n",
      "      - 2.0\n",
      "      - 0.0\n",
      "      - 2.0\n",
      "      - 4.0\n",
      "      - -1.0\n",
      "      - 2.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - -7.0\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -4.0\n",
      "      - -2.0\n",
      "      - 4.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - -6.0\n",
      "      - 4.0\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - 2.0\n",
      "      - 2.0\n",
      "      - -5.0\n",
      "      - 1.0\n",
      "      - -6.0\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - 5.0\n",
      "      - -3.0\n",
      "      - -4.0\n",
      "      - 5.0\n",
      "      - 1.0\n",
      "      - 4.0\n",
      "      - -1.0\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - 1.0\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - -5.0\n",
      "      - 3.0\n",
      "      - -0.5\n",
      "      - -7.0\n",
      "      - 1.0\n",
      "      - -3.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - -3.0\n",
      "      - 1.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      ppo_policy_1: 7.0\n",
      "      ppo_policy_2: 7.0\n",
      "    policy_reward_mean:\n",
      "      ppo_policy_1: 0.12\n",
      "      ppo_policy_2: -0.12\n",
      "    policy_reward_min:\n",
      "      ppo_policy_1: -7.0\n",
      "      ppo_policy_2: -7.0\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 0.38006329777264836\n",
      "      mean_inference_ms: 4.954032223634045\n",
      "      mean_processing_ms: 2.1157981169344207\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.919204831123352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030920786783099174\n",
      "        model: {}\n",
      "        policy_loss: -0.05328305810689926\n",
      "        total_loss: 8.56913948059082\n",
      "        vf_explained_var: 0.11159846186637878\n",
      "        vf_loss: 8.613146781921387\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9251587986946106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02881661430001259\n",
      "        model: {}\n",
      "        policy_loss: -0.04394955560564995\n",
      "        total_loss: 8.821915626525879\n",
      "        vf_explained_var: 0.058307062834501266\n",
      "        vf_loss: 8.857221603393555\n",
      "    num_steps_sampled: 8010\n",
      "    num_steps_trained: 8010\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.32093023255813\n",
      "    ram_util_percent: 67.08837209302325\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: 0.03080168776371308\n",
      "    ppo_policy_2: -0.03080168776371308\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.5515690564399588\n",
      "    mean_inference_ms: 5.123116019970915\n",
      "    mean_processing_ms: 3.2099502876777537\n",
      "  time_since_restore: 70.55984854698181\n",
      "  time_this_iter_s: 29.512132167816162\n",
      "  time_total_s: 70.55984854698181\n",
      "  timers:\n",
      "    learn_throughput: 294.006\n",
      "    learn_time_ms: 13622.179\n",
      "    load_throughput: 4639.216\n",
      "    load_time_ms: 863.292\n",
      "    sample_throughput: 218.537\n",
      "    sample_time_ms: 18326.39\n",
      "    update_time_ms: 25.07\n",
      "  timestamp: 1605684349\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8010\n",
      "  training_iteration: 2\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:25:57,248\tWARNING util.py:137 -- The `process_trial` operation took 0.6463236808776855 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:25:57,333\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         70.5598</td><td style=\"text-align: right;\">8010</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: 0.17578125\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: -0.17578125\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 3.912109375\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1024\n",
      "  episodes_total: 3785\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8403305411338806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015164325945079327\n",
      "        model: {}\n",
      "        policy_loss: -0.02859964780509472\n",
      "        total_loss: 10.162201881408691\n",
      "        vf_explained_var: 0.15490694344043732\n",
      "        vf_loss: 10.183978080749512\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8506857752799988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01641855761408806\n",
      "        model: {}\n",
      "        policy_loss: -0.029984818771481514\n",
      "        total_loss: 10.47517204284668\n",
      "        vf_explained_var: 0.1381680816411972\n",
      "        vf_loss: 10.49776840209961\n",
      "    num_steps_sampled: 12016\n",
      "    num_steps_trained: 12016\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.60754716981131\n",
      "    ram_util_percent: 66.79056603773586\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: 0.17578125\n",
      "    ppo_policy_2: -0.17578125\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.5274511887944394\n",
      "    mean_inference_ms: 4.883591627122309\n",
      "    mean_processing_ms: 2.968994145167593\n",
      "  time_since_restore: 99.40981221199036\n",
      "  time_this_iter_s: 28.849963665008545\n",
      "  time_total_s: 99.40981221199036\n",
      "  timers:\n",
      "    learn_throughput: 296.378\n",
      "    learn_time_ms: 13514.284\n",
      "    load_throughput: 6909.245\n",
      "    load_time_ms: 579.706\n",
      "    sample_throughput: 231.858\n",
      "    sample_time_ms: 17274.906\n",
      "    update_time_ms: 24.401\n",
      "  timestamp: 1605684386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12016\n",
      "  training_iteration: 3\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:26:27,136\tWARNING util.py:137 -- The `process_trial` operation took 0.6186459064483643 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:26:27,242\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         99.4098</td><td style=\"text-align: right;\">12016</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: 0.1846076458752515\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: -0.1846076458752515\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-26-56\n",
      "  done: false\n",
      "  episode_len_mean: 4.027162977867203\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 994\n",
      "  episodes_total: 4779\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      player_1_score_max: 7.0\n",
      "      player_1_score_mean: 0.09\n",
      "      player_1_score_min: -7.0\n",
      "      player_2_score_max: 7.0\n",
      "      player_2_score_mean: -0.09\n",
      "      player_2_score_min: -7.0\n",
      "    episode_len_mean: 4.18\n",
      "    episode_reward_max: 0.0\n",
      "    episode_reward_mean: 0.0\n",
      "    episode_reward_min: 0.0\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 2\n",
      "      - 4\n",
      "      - 2\n",
      "      - 4\n",
      "      - 1\n",
      "      - 6\n",
      "      - 6\n",
      "      - 1\n",
      "      - 4\n",
      "      - 1\n",
      "      - 4\n",
      "      - 5\n",
      "      - 6\n",
      "      - 4\n",
      "      - 6\n",
      "      - 6\n",
      "      - 4\n",
      "      - 1\n",
      "      - 2\n",
      "      - 5\n",
      "      - 4\n",
      "      - 2\n",
      "      - 3\n",
      "      - 8\n",
      "      - 1\n",
      "      - 6\n",
      "      - 5\n",
      "      - 5\n",
      "      - 5\n",
      "      - 1\n",
      "      - 5\n",
      "      - 5\n",
      "      - 6\n",
      "      - 6\n",
      "      - 8\n",
      "      - 5\n",
      "      - 4\n",
      "      - 7\n",
      "      - 4\n",
      "      - 4\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 1\n",
      "      - 5\n",
      "      - 5\n",
      "      - 3\n",
      "      - 6\n",
      "      - 1\n",
      "      - 5\n",
      "      - 1\n",
      "      - 6\n",
      "      - 6\n",
      "      - 4\n",
      "      - 2\n",
      "      - 5\n",
      "      - 3\n",
      "      - 5\n",
      "      - 2\n",
      "      - 4\n",
      "      - 6\n",
      "      - 1\n",
      "      - 1\n",
      "      - 6\n",
      "      - 5\n",
      "      - 4\n",
      "      - 5\n",
      "      - 6\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 6\n",
      "      - 4\n",
      "      - 4\n",
      "      - 3\n",
      "      - 1\n",
      "      - 4\n",
      "      - 5\n",
      "      - 5\n",
      "      - 7\n",
      "      - 7\n",
      "      - 7\n",
      "      - 4\n",
      "      - 5\n",
      "      - 7\n",
      "      - 4\n",
      "      - 5\n",
      "      - 2\n",
      "      - 5\n",
      "      - 6\n",
      "      - 1\n",
      "      - 6\n",
      "      - 1\n",
      "      - 5\n",
      "      - 1\n",
      "      - 7\n",
      "      - 5\n",
      "      - 1\n",
      "      - 2\n",
      "      - 4\n",
      "      episode_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      policy_ppo_policy_1_reward:\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -5.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - -0.5\n",
      "      - -4.0\n",
      "      - 5.0\n",
      "      - 3.0\n",
      "      - 2.0\n",
      "      - 4.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - -5.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - -2.0\n",
      "      - -5.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - -3.0\n",
      "      - 5.0\n",
      "      - 6.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - 5.0\n",
      "      - -3.0\n",
      "      - 7.0\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - -4.0\n",
      "      - 5.0\n",
      "      - -1.0\n",
      "      - 2.0\n",
      "      - 0.0\n",
      "      - 6.0\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - -6.0\n",
      "      - -1.0\n",
      "      - -6.0\n",
      "      - 0.5\n",
      "      - -5.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - -2.0\n",
      "      - 1.0\n",
      "      - 6.0\n",
      "      - -2.0\n",
      "      - 6.0\n",
      "      - 1.0\n",
      "      - -2.0\n",
      "      - -3.0\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - 3.0\n",
      "      - -2.0\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - 2.0\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - 3.0\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - 2.0\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - 7.0\n",
      "      - -7.0\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -7.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 5.0\n",
      "      - -6.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - 0.5\n",
      "      - -6.0\n",
      "      - -0.5\n",
      "      - 7.0\n",
      "      - 2.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      policy_ppo_policy_2_reward:\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 5.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - 0.5\n",
      "      - 4.0\n",
      "      - -5.0\n",
      "      - -3.0\n",
      "      - -2.0\n",
      "      - -4.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 5.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      - 5.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - 3.0\n",
      "      - -5.0\n",
      "      - -6.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - -5.0\n",
      "      - 3.0\n",
      "      - -7.0\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - 4.0\n",
      "      - -5.0\n",
      "      - 1.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - -6.0\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - 6.0\n",
      "      - 1.0\n",
      "      - 6.0\n",
      "      - -0.5\n",
      "      - 5.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - 2.0\n",
      "      - -1.0\n",
      "      - -6.0\n",
      "      - 2.0\n",
      "      - -6.0\n",
      "      - -1.0\n",
      "      - 2.0\n",
      "      - 3.0\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - -3.0\n",
      "      - 2.0\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - -2.0\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - -3.0\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - -2.0\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - -7.0\n",
      "      - 7.0\n",
      "      - -7.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -5.0\n",
      "      - 6.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - -0.5\n",
      "      - 6.0\n",
      "      - 0.5\n",
      "      - -7.0\n",
      "      - -2.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - -2.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      ppo_policy_1: 7.0\n",
      "      ppo_policy_2: 7.0\n",
      "    policy_reward_mean:\n",
      "      ppo_policy_1: 0.09\n",
      "      ppo_policy_2: -0.09\n",
      "    policy_reward_min:\n",
      "      ppo_policy_1: -7.0\n",
      "      ppo_policy_2: -7.0\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 0.3844994584816973\n",
      "      mean_inference_ms: 4.261319115941003\n",
      "      mean_processing_ms: 2.0740916746547233\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7868013381958008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008579070679843426\n",
      "        model: {}\n",
      "        policy_loss: -0.021341638639569283\n",
      "        total_loss: 9.716137886047363\n",
      "        vf_explained_var: 0.16418582201004028\n",
      "        vf_loss: 9.733617782592773\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7972150444984436\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012758135795593262\n",
      "        model: {}\n",
      "        policy_loss: -0.026341838762164116\n",
      "        total_loss: 9.169187545776367\n",
      "        vf_explained_var: 0.2072102576494217\n",
      "        vf_loss: 9.189788818359375\n",
      "    num_steps_sampled: 16019\n",
      "    num_steps_trained: 16019\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.77674418604651\n",
      "    ram_util_percent: 66.46046511627907\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: 0.1846076458752515\n",
      "    ppo_policy_2: -0.1846076458752515\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.5274492533369881\n",
      "    mean_inference_ms: 4.914538607737426\n",
      "    mean_processing_ms: 2.92404621652341\n",
      "  time_since_restore: 128.40727400779724\n",
      "  time_this_iter_s: 28.997461795806885\n",
      "  time_total_s: 128.40727400779724\n",
      "  timers:\n",
      "    learn_throughput: 306.901\n",
      "    learn_time_ms: 13048.98\n",
      "    load_throughput: 9160.827\n",
      "    load_time_ms: 437.16\n",
      "    sample_throughput: 232.365\n",
      "    sample_time_ms: 17234.718\n",
      "    update_time_ms: 23.815\n",
      "  timestamp: 1605684416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16019\n",
      "  training_iteration: 4\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:27:00,535\tWARNING util.py:137 -- The `process_trial` operation took 0.5742244720458984 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:27:00,621\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         128.407</td><td style=\"text-align: right;\">16019</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: 0.26597938144329897\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: -0.26597938144329897\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 4.134020618556701\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 970\n",
      "  episodes_total: 5749\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7333629131317139\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009003384970128536\n",
      "        model: {}\n",
      "        policy_loss: -0.020643673837184906\n",
      "        total_loss: 10.632890701293945\n",
      "        vf_explained_var: 0.24375443160533905\n",
      "        vf_loss: 10.649480819702148\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7847314476966858\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00865980889648199\n",
      "        model: {}\n",
      "        policy_loss: -0.025620825588703156\n",
      "        total_loss: 10.644291877746582\n",
      "        vf_explained_var: 0.22917978465557098\n",
      "        vf_loss: 10.666015625\n",
      "    num_steps_sampled: 20027\n",
      "    num_steps_trained: 20027\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.83958333333334\n",
      "    ram_util_percent: 66.36666666666666\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: 0.26597938144329897\n",
      "    ppo_policy_2: -0.26597938144329897\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.5196870077046\n",
      "    mean_inference_ms: 4.856537265866862\n",
      "    mean_processing_ms: 2.850484811261048\n",
      "  time_since_restore: 158.34140014648438\n",
      "  time_this_iter_s: 29.934126138687134\n",
      "  time_total_s: 158.34140014648438\n",
      "  timers:\n",
      "    learn_throughput: 302.854\n",
      "    learn_time_ms: 13225.524\n",
      "    load_throughput: 11381.113\n",
      "    load_time_ms: 351.934\n",
      "    sample_throughput: 236.483\n",
      "    sample_time_ms: 16937.365\n",
      "    update_time_ms: 22.962\n",
      "  timestamp: 1605684450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20027\n",
      "  training_iteration: 5\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:27:31,340\tWARNING util.py:137 -- The `process_trial` operation took 0.5344386100769043 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:27:31,434\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         158.341</td><td style=\"text-align: right;\">20027</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: 0.027214514407684097\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: -0.027214514407684097\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-27-58\n",
      "  done: false\n",
      "  episode_len_mean: 4.273212379935966\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 937\n",
      "  episodes_total: 6686\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      player_1_score_max: 7.0\n",
      "      player_1_score_mean: -0.065\n",
      "      player_1_score_min: -7.0\n",
      "      player_2_score_max: 7.0\n",
      "      player_2_score_mean: 0.065\n",
      "      player_2_score_min: -7.0\n",
      "    episode_len_mean: 4.13\n",
      "    episode_reward_max: 0.0\n",
      "    episode_reward_mean: 0.0\n",
      "    episode_reward_min: 0.0\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 6\n",
      "      - 1\n",
      "      - 6\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 5\n",
      "      - 1\n",
      "      - 1\n",
      "      - 2\n",
      "      - 5\n",
      "      - 4\n",
      "      - 1\n",
      "      - 4\n",
      "      - 6\n",
      "      - 3\n",
      "      - 5\n",
      "      - 5\n",
      "      - 2\n",
      "      - 5\n",
      "      - 3\n",
      "      - 2\n",
      "      - 6\n",
      "      - 7\n",
      "      - 5\n",
      "      - 5\n",
      "      - 7\n",
      "      - 1\n",
      "      - 7\n",
      "      - 6\n",
      "      - 1\n",
      "      - 5\n",
      "      - 5\n",
      "      - 6\n",
      "      - 7\n",
      "      - 6\n",
      "      - 5\n",
      "      - 6\n",
      "      - 5\n",
      "      - 4\n",
      "      - 1\n",
      "      - 5\n",
      "      - 4\n",
      "      - 5\n",
      "      - 2\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 2\n",
      "      - 5\n",
      "      - 5\n",
      "      - 1\n",
      "      - 5\n",
      "      - 4\n",
      "      - 2\n",
      "      - 4\n",
      "      - 2\n",
      "      - 2\n",
      "      - 1\n",
      "      - 5\n",
      "      - 2\n",
      "      - 1\n",
      "      - 5\n",
      "      - 7\n",
      "      - 3\n",
      "      - 5\n",
      "      - 2\n",
      "      - 1\n",
      "      - 2\n",
      "      - 5\n",
      "      - 1\n",
      "      - 6\n",
      "      - 4\n",
      "      - 1\n",
      "      - 4\n",
      "      - 4\n",
      "      - 4\n",
      "      - 4\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 6\n",
      "      - 6\n",
      "      - 5\n",
      "      - 6\n",
      "      - 4\n",
      "      - 4\n",
      "      - 4\n",
      "      - 6\n",
      "      - 4\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 7\n",
      "      - 6\n",
      "      - 5\n",
      "      - 5\n",
      "      - 3\n",
      "      - 6\n",
      "      - 3\n",
      "      episode_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      policy_ppo_policy_1_reward:\n",
      "      - 6.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - -3.0\n",
      "      - -6.0\n",
      "      - 2.0\n",
      "      - -6.0\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      - 4.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - -2.0\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - -7.0\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - -6.0\n",
      "      - 0.5\n",
      "      - -5.0\n",
      "      - 3.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - 5.0\n",
      "      - 7.0\n",
      "      - 7.0\n",
      "      - -3.0\n",
      "      - -6.0\n",
      "      - 2.0\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - 5.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - -6.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - -5.0\n",
      "      - 2.0\n",
      "      - 0.5\n",
      "      - -5.0\n",
      "      - 4.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 6.0\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - -0.5\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - 1.0\n",
      "      - -1.0\n",
      "      - 3.0\n",
      "      - 6.0\n",
      "      - 1.0\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - -3.0\n",
      "      - 4.0\n",
      "      - 2.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - 4.0\n",
      "      - -5.0\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - -6.0\n",
      "      - 3.0\n",
      "      - -5.0\n",
      "      - 1.0\n",
      "      - -6.0\n",
      "      - 1.0\n",
      "      policy_ppo_policy_2_reward:\n",
      "      - -6.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - 3.0\n",
      "      - 6.0\n",
      "      - -2.0\n",
      "      - 6.0\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "      - -1.0\n",
      "      - -2.0\n",
      "      - -4.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - 2.0\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - 7.0\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - 6.0\n",
      "      - -0.5\n",
      "      - 5.0\n",
      "      - -3.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - -5.0\n",
      "      - -7.0\n",
      "      - -7.0\n",
      "      - 3.0\n",
      "      - 6.0\n",
      "      - -2.0\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - -5.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - 6.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 5.0\n",
      "      - -2.0\n",
      "      - -0.5\n",
      "      - 5.0\n",
      "      - -4.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -6.0\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - 0.5\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - 2.0\n",
      "      - -1.0\n",
      "      - 1.0\n",
      "      - -3.0\n",
      "      - -6.0\n",
      "      - -1.0\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - 3.0\n",
      "      - -4.0\n",
      "      - -2.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - -4.0\n",
      "      - 5.0\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - 6.0\n",
      "      - -3.0\n",
      "      - 5.0\n",
      "      - -1.0\n",
      "      - 6.0\n",
      "      - -1.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      ppo_policy_1: 7.0\n",
      "      ppo_policy_2: 7.0\n",
      "    policy_reward_mean:\n",
      "      ppo_policy_1: -0.065\n",
      "      ppo_policy_2: 0.065\n",
      "    policy_reward_min:\n",
      "      ppo_policy_1: -7.0\n",
      "      ppo_policy_2: -7.0\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 0.4005976197383418\n",
      "      mean_inference_ms: 4.147629283168207\n",
      "      mean_processing_ms: 2.1340854787787675\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6897315979003906\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011240314692258835\n",
      "        model: {}\n",
      "        policy_loss: -0.02062097378075123\n",
      "        total_loss: 10.461411476135254\n",
      "        vf_explained_var: 0.2914339303970337\n",
      "        vf_loss: 10.476975440979004\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7502431869506836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009091022424399853\n",
      "        model: {}\n",
      "        policy_loss: -0.020334817469120026\n",
      "        total_loss: 10.603475570678711\n",
      "        vf_explained_var: 0.267192542552948\n",
      "        vf_loss: 10.619718551635742\n",
      "    num_steps_sampled: 24033\n",
      "    num_steps_trained: 24033\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.44250000000001\n",
      "    ram_util_percent: 66.3425\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: 0.027214514407684097\n",
      "    ppo_policy_2: -0.027214514407684097\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.5056851394563635\n",
      "    mean_inference_ms: 4.72063840334799\n",
      "    mean_processing_ms: 2.741589107241713\n",
      "  time_since_restore: 185.70813274383545\n",
      "  time_this_iter_s: 27.366732597351074\n",
      "  time_total_s: 185.70813274383545\n",
      "  timers:\n",
      "    learn_throughput: 302.504\n",
      "    learn_time_ms: 13241.135\n",
      "    load_throughput: 13507.546\n",
      "    load_time_ms: 296.538\n",
      "    sample_throughput: 244.194\n",
      "    sample_time_ms: 16402.948\n",
      "    update_time_ms: 23.731\n",
      "  timestamp: 1605684478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24033\n",
      "  training_iteration: 6\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:28:03,592\tWARNING util.py:137 -- The `process_trial` operation took 0.5616824626922607 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:28:03,682\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         185.708</td><td style=\"text-align: right;\">24033</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: 0.027777777777777776\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: -0.027777777777777776\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-28-31\n",
      "  done: false\n",
      "  episode_len_mean: 4.1928721174004195\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 954\n",
      "  episodes_total: 7640\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6488468050956726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01049336139112711\n",
      "        model: {}\n",
      "        policy_loss: -0.01556729432195425\n",
      "        total_loss: 10.230477333068848\n",
      "        vf_explained_var: 0.263813853263855\n",
      "        vf_loss: 10.241321563720703\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7357017397880554\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008324232883751392\n",
      "        model: {}\n",
      "        policy_loss: -0.025446396321058273\n",
      "        total_loss: 9.97358512878418\n",
      "        vf_explained_var: 0.2765780985355377\n",
      "        vf_loss: 9.995285034179688\n",
      "    num_steps_sampled: 28034\n",
      "    num_steps_trained: 28034\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.58510638297871\n",
      "    ram_util_percent: 66.21276595744679\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: 0.027777777777777776\n",
      "    ppo_policy_2: -0.027777777777777776\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.5035010797689518\n",
      "    mean_inference_ms: 4.700531286157357\n",
      "    mean_processing_ms: 2.711129040621244\n",
      "  time_since_restore: 213.4949185848236\n",
      "  time_this_iter_s: 27.78678584098816\n",
      "  time_total_s: 213.4949185848236\n",
      "  timers:\n",
      "    learn_throughput: 306.851\n",
      "    learn_time_ms: 13051.458\n",
      "    load_throughput: 15628.59\n",
      "    load_time_ms: 256.252\n",
      "    sample_throughput: 245.894\n",
      "    sample_time_ms: 16286.954\n",
      "    update_time_ms: 23.348\n",
      "  timestamp: 1605684511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28034\n",
      "  training_iteration: 7\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:28:32,132\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         213.495</td><td style=\"text-align: right;\">28034</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: -0.06321243523316063\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: 0.06321243523316063\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-28-57\n",
      "  done: false\n",
      "  episode_len_mean: 4.147150259067358\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 965\n",
      "  episodes_total: 8605\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      player_1_score_max: 7.0\n",
      "      player_1_score_mean: 0.695\n",
      "      player_1_score_min: -7.0\n",
      "      player_2_score_max: 7.0\n",
      "      player_2_score_mean: -0.695\n",
      "      player_2_score_min: -7.0\n",
      "    episode_len_mean: 4.05\n",
      "    episode_reward_max: 0.0\n",
      "    episode_reward_mean: 0.0\n",
      "    episode_reward_min: 0.0\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1\n",
      "      - 4\n",
      "      - 4\n",
      "      - 3\n",
      "      - 7\n",
      "      - 6\n",
      "      - 4\n",
      "      - 5\n",
      "      - 6\n",
      "      - 1\n",
      "      - 2\n",
      "      - 1\n",
      "      - 6\n",
      "      - 2\n",
      "      - 5\n",
      "      - 4\n",
      "      - 1\n",
      "      - 5\n",
      "      - 4\n",
      "      - 4\n",
      "      - 7\n",
      "      - 3\n",
      "      - 5\n",
      "      - 6\n",
      "      - 5\n",
      "      - 1\n",
      "      - 1\n",
      "      - 1\n",
      "      - 6\n",
      "      - 6\n",
      "      - 6\n",
      "      - 4\n",
      "      - 5\n",
      "      - 5\n",
      "      - 5\n",
      "      - 6\n",
      "      - 4\n",
      "      - 6\n",
      "      - 2\n",
      "      - 7\n",
      "      - 4\n",
      "      - 3\n",
      "      - 5\n",
      "      - 4\n",
      "      - 1\n",
      "      - 1\n",
      "      - 6\n",
      "      - 1\n",
      "      - 5\n",
      "      - 6\n",
      "      - 4\n",
      "      - 1\n",
      "      - 1\n",
      "      - 6\n",
      "      - 5\n",
      "      - 5\n",
      "      - 1\n",
      "      - 6\n",
      "      - 5\n",
      "      - 4\n",
      "      - 6\n",
      "      - 3\n",
      "      - 3\n",
      "      - 3\n",
      "      - 4\n",
      "      - 2\n",
      "      - 6\n",
      "      - 2\n",
      "      - 6\n",
      "      - 3\n",
      "      - 4\n",
      "      - 7\n",
      "      - 4\n",
      "      - 1\n",
      "      - 5\n",
      "      - 5\n",
      "      - 1\n",
      "      - 5\n",
      "      - 1\n",
      "      - 5\n",
      "      - 5\n",
      "      - 1\n",
      "      - 5\n",
      "      - 5\n",
      "      - 6\n",
      "      - 5\n",
      "      - 6\n",
      "      - 4\n",
      "      - 4\n",
      "      - 1\n",
      "      - 5\n",
      "      - 3\n",
      "      - 4\n",
      "      - 4\n",
      "      - 1\n",
      "      - 6\n",
      "      - 7\n",
      "      - 6\n",
      "      - 6\n",
      "      - 5\n",
      "      episode_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      policy_ppo_policy_1_reward:\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 7.0\n",
      "      - 5.0\n",
      "      - -2.0\n",
      "      - 5.0\n",
      "      - -5.0\n",
      "      - -0.5\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - -5.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - -4.0\n",
      "      - -1.0\n",
      "      - -5.0\n",
      "      - -1.0\n",
      "      - 3.0\n",
      "      - 6.0\n",
      "      - -5.0\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - 7.0\n",
      "      - -3.0\n",
      "      - -4.0\n",
      "      - 3.0\n",
      "      - 2.0\n",
      "      - 6.0\n",
      "      - 0.0\n",
      "      - 3.0\n",
      "      - -3.0\n",
      "      - 1.0\n",
      "      - 5.0\n",
      "      - -2.0\n",
      "      - -1.0\n",
      "      - 5.0\n",
      "      - 4.0\n",
      "      - -0.5\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 0.5\n",
      "      - -7.0\n",
      "      - 3.0\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - 3.0\n",
      "      - 4.0\n",
      "      - 5.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 7.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      - -2.0\n",
      "      - 4.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - 0.5\n",
      "      - -6.0\n",
      "      - 0.5\n",
      "      - 6.0\n",
      "      - -3.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - 4.0\n",
      "      - 7.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - -2.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - 5.0\n",
      "      - -3.0\n",
      "      - -4.0\n",
      "      policy_ppo_policy_2_reward:\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - -7.0\n",
      "      - -5.0\n",
      "      - 2.0\n",
      "      - -5.0\n",
      "      - 5.0\n",
      "      - 0.5\n",
      "      - -1.0\n",
      "      - 0.5\n",
      "      - 5.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - 1.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - 4.0\n",
      "      - 1.0\n",
      "      - 5.0\n",
      "      - 1.0\n",
      "      - -3.0\n",
      "      - -6.0\n",
      "      - 5.0\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - -7.0\n",
      "      - 3.0\n",
      "      - 4.0\n",
      "      - -3.0\n",
      "      - -2.0\n",
      "      - -6.0\n",
      "      - 0.0\n",
      "      - -3.0\n",
      "      - 3.0\n",
      "      - -1.0\n",
      "      - -5.0\n",
      "      - 2.0\n",
      "      - 1.0\n",
      "      - -5.0\n",
      "      - -4.0\n",
      "      - 0.5\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 3.0\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - -0.5\n",
      "      - 7.0\n",
      "      - -3.0\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - -3.0\n",
      "      - -3.0\n",
      "      - -4.0\n",
      "      - -5.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -7.0\n",
      "      - -1.0\n",
      "      - -3.0\n",
      "      - 2.0\n",
      "      - -4.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - -0.5\n",
      "      - 6.0\n",
      "      - -0.5\n",
      "      - -6.0\n",
      "      - 3.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -3.0\n",
      "      - -5.0\n",
      "      - -4.0\n",
      "      - -7.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - 2.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - -5.0\n",
      "      - 3.0\n",
      "      - 4.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      ppo_policy_1: 7.0\n",
      "      ppo_policy_2: 7.0\n",
      "    policy_reward_mean:\n",
      "      ppo_policy_1: 0.695\n",
      "      ppo_policy_2: -0.695\n",
      "    policy_reward_min:\n",
      "      ppo_policy_1: -7.0\n",
      "      ppo_policy_2: -7.0\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 0.3934242561751721\n",
      "      mean_inference_ms: 3.9897962820296176\n",
      "      mean_processing_ms: 2.0764321088790894\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6255425810813904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006460756994783878\n",
      "        model: {}\n",
      "        policy_loss: -0.018842827528715134\n",
      "        total_loss: 9.825186729431152\n",
      "        vf_explained_var: 0.2715732753276825\n",
      "        vf_loss: 9.841121673583984\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.697932243347168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007021911442279816\n",
      "        model: {}\n",
      "        policy_loss: -0.01003097090870142\n",
      "        total_loss: 9.714737892150879\n",
      "        vf_explained_var: 0.2689605951309204\n",
      "        vf_loss: 9.72160816192627\n",
      "    num_steps_sampled: 32035\n",
      "    num_steps_trained: 32035\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.10833333333333\n",
      "    ram_util_percent: 66.11666666666665\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: -0.06321243523316063\n",
      "    ppo_policy_2: 0.06321243523316063\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.49376522771714154\n",
      "    mean_inference_ms: 4.598866942851371\n",
      "    mean_processing_ms: 2.637723899752661\n",
      "  time_since_restore: 238.42893433570862\n",
      "  time_this_iter_s: 24.93401575088501\n",
      "  time_total_s: 238.42893433570862\n",
      "  timers:\n",
      "    learn_throughput: 311.72\n",
      "    learn_time_ms: 12846.084\n",
      "    load_throughput: 17754.915\n",
      "    load_time_ms: 225.536\n",
      "    sample_throughput: 251.813\n",
      "    sample_time_ms: 15902.152\n",
      "    update_time_ms: 22.999\n",
      "  timestamp: 1605684537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32035\n",
      "  training_iteration: 8\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:29:01,357\tWARNING util.py:137 -- The `process_trial` operation took 0.667330265045166 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:29:01,439\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         238.429</td><td style=\"text-align: right;\">32035</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: -0.06387225548902195\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: 0.06387225548902195\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 3.9880239520958085\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1002\n",
      "  episodes_total: 9607\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5934880375862122\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00944802537560463\n",
      "        model: {}\n",
      "        policy_loss: -0.02367359772324562\n",
      "        total_loss: 9.285711288452148\n",
      "        vf_explained_var: 0.32868272066116333\n",
      "        vf_loss: 9.305133819580078\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6613041162490845\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013745157979428768\n",
      "        model: {}\n",
      "        policy_loss: -0.026302453130483627\n",
      "        total_loss: 9.003189086914062\n",
      "        vf_explained_var: 0.32972344756126404\n",
      "        vf_loss: 9.023306846618652\n",
      "    num_steps_sampled: 36036\n",
      "    num_steps_trained: 36036\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.27441860465116\n",
      "    ram_util_percent: 66.22558139534884\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: -0.06387225548902195\n",
      "    ppo_policy_2: 0.06387225548902195\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.48764529952019503\n",
      "    mean_inference_ms: 4.534984504579154\n",
      "    mean_processing_ms: 2.5941384058338746\n",
      "  time_since_restore: 263.82987904548645\n",
      "  time_this_iter_s: 25.400944709777832\n",
      "  time_total_s: 263.82987904548645\n",
      "  timers:\n",
      "    learn_throughput: 315.816\n",
      "    learn_time_ms: 12678.269\n",
      "    load_throughput: 19851.051\n",
      "    load_time_ms: 201.702\n",
      "    sample_throughput: 255.514\n",
      "    sample_time_ms: 15670.347\n",
      "    update_time_ms: 22.839\n",
      "  timestamp: 1605684566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36036\n",
      "  training_iteration: 9\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:29:27,560\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          263.83</td><td style=\"text-align: right;\">36036</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_leduc-holdem_ec3d3_00000:\n",
      "  callback_ok: true\n",
      "  custom_metrics:\n",
      "    player_1_score_max: 7.0\n",
      "    player_1_score_mean: -0.14990328820116053\n",
      "    player_1_score_min: -7.0\n",
      "    player_2_score_max: 7.0\n",
      "    player_2_score_mean: 0.14990328820116053\n",
      "    player_2_score_min: -7.0\n",
      "  date: 2020-11-18_08-29-53\n",
      "  done: true\n",
      "  episode_len_mean: 3.881044487427466\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1034\n",
      "  episodes_total: 10641\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      player_1_score_max: 7.0\n",
      "      player_1_score_mean: 0.0\n",
      "      player_1_score_min: -7.0\n",
      "      player_2_score_max: 7.0\n",
      "      player_2_score_mean: 0.0\n",
      "      player_2_score_min: -7.0\n",
      "    episode_len_mean: 3.34\n",
      "    episode_reward_max: 0.0\n",
      "    episode_reward_mean: 0.0\n",
      "    episode_reward_min: 0.0\n",
      "    episodes_this_iter: 100\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1\n",
      "      - 4\n",
      "      - 7\n",
      "      - 4\n",
      "      - 2\n",
      "      - 4\n",
      "      - 6\n",
      "      - 5\n",
      "      - 1\n",
      "      - 1\n",
      "      - 5\n",
      "      - 1\n",
      "      - 1\n",
      "      - 4\n",
      "      - 1\n",
      "      - 1\n",
      "      - 6\n",
      "      - 6\n",
      "      - 1\n",
      "      - 1\n",
      "      - 2\n",
      "      - 1\n",
      "      - 3\n",
      "      - 5\n",
      "      - 3\n",
      "      - 4\n",
      "      - 1\n",
      "      - 2\n",
      "      - 6\n",
      "      - 1\n",
      "      - 5\n",
      "      - 1\n",
      "      - 6\n",
      "      - 1\n",
      "      - 1\n",
      "      - 4\n",
      "      - 1\n",
      "      - 1\n",
      "      - 4\n",
      "      - 1\n",
      "      - 1\n",
      "      - 1\n",
      "      - 5\n",
      "      - 1\n",
      "      - 4\n",
      "      - 4\n",
      "      - 1\n",
      "      - 6\n",
      "      - 5\n",
      "      - 5\n",
      "      - 1\n",
      "      - 5\n",
      "      - 6\n",
      "      - 4\n",
      "      - 2\n",
      "      - 6\n",
      "      - 6\n",
      "      - 5\n",
      "      - 4\n",
      "      - 5\n",
      "      - 5\n",
      "      - 4\n",
      "      - 4\n",
      "      - 4\n",
      "      - 1\n",
      "      - 2\n",
      "      - 6\n",
      "      - 2\n",
      "      - 3\n",
      "      - 1\n",
      "      - 2\n",
      "      - 3\n",
      "      - 4\n",
      "      - 4\n",
      "      - 4\n",
      "      - 5\n",
      "      - 4\n",
      "      - 4\n",
      "      - 2\n",
      "      - 2\n",
      "      - 7\n",
      "      - 1\n",
      "      - 5\n",
      "      - 7\n",
      "      - 1\n",
      "      - 4\n",
      "      - 4\n",
      "      - 5\n",
      "      - 4\n",
      "      - 4\n",
      "      - 5\n",
      "      - 1\n",
      "      - 5\n",
      "      - 4\n",
      "      - 1\n",
      "      - 5\n",
      "      - 1\n",
      "      - 2\n",
      "      - 4\n",
      "      - 6\n",
      "      episode_reward:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      policy_ppo_policy_1_reward:\n",
      "      - 0.5\n",
      "      - 4.0\n",
      "      - -5.0\n",
      "      - -2.0\n",
      "      - 1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 5.0\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - 0.5\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 0.5\n",
      "      - -6.0\n",
      "      - -5.0\n",
      "      - 0.5\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 5.0\n",
      "      - 1.0\n",
      "      - -4.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 7.0\n",
      "      - -0.5\n",
      "      - 2.0\n",
      "      - 0.5\n",
      "      - -7.0\n",
      "      - 0.5\n",
      "      - -0.5\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - 2.0\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - 3.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - -7.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - -5.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -2.0\n",
      "      - -0.5\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 2.0\n",
      "      - 3.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -5.0\n",
      "      - 0.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - -1.0\n",
      "      - -5.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -5.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 4.0\n",
      "      - -1.0\n",
      "      - 4.0\n",
      "      - 2.0\n",
      "      - 0.5\n",
      "      - 5.0\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - -0.5\n",
      "      - -1.0\n",
      "      - 4.0\n",
      "      - 6.0\n",
      "      policy_ppo_policy_2_reward:\n",
      "      - -0.5\n",
      "      - -4.0\n",
      "      - 5.0\n",
      "      - 2.0\n",
      "      - -1.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - -5.0\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - -0.5\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - -0.5\n",
      "      - 6.0\n",
      "      - 5.0\n",
      "      - -0.5\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - -5.0\n",
      "      - -1.0\n",
      "      - 4.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - -7.0\n",
      "      - 0.5\n",
      "      - -2.0\n",
      "      - -0.5\n",
      "      - 7.0\n",
      "      - -0.5\n",
      "      - 0.5\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - -2.0\n",
      "      - 2.0\n",
      "      - -0.5\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - -3.0\n",
      "      - 0.5\n",
      "      - 3.0\n",
      "      - 7.0\n",
      "      - -1.0\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 5.0\n",
      "      - 3.0\n",
      "      - 1.0\n",
      "      - 0.0\n",
      "      - 2.0\n",
      "      - 0.5\n",
      "      - -1.0\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -2.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - -2.0\n",
      "      - -3.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 5.0\n",
      "      - 0.0\n",
      "      - -3.0\n",
      "      - -1.0\n",
      "      - 1.0\n",
      "      - 5.0\n",
      "      - 0.5\n",
      "      - 0.0\n",
      "      - 5.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - -1.0\n",
      "      - -4.0\n",
      "      - 1.0\n",
      "      - -4.0\n",
      "      - -2.0\n",
      "      - -0.5\n",
      "      - -5.0\n",
      "      - -1.0\n",
      "      - -0.5\n",
      "      - 0.0\n",
      "      - 0.5\n",
      "      - 1.0\n",
      "      - -4.0\n",
      "      - -6.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      ppo_policy_1: 7.0\n",
      "      ppo_policy_2: 7.0\n",
      "    policy_reward_mean:\n",
      "      ppo_policy_1: 0.0\n",
      "      ppo_policy_2: 0.0\n",
      "    policy_reward_min:\n",
      "      ppo_policy_1: -7.0\n",
      "      ppo_policy_2: -7.0\n",
      "    sampler_perf:\n",
      "      mean_env_wait_ms: 0.3921826761348566\n",
      "      mean_inference_ms: 3.894903046189829\n",
      "      mean_processing_ms: 2.0925092794160274\n",
      "  experiment_id: 5300f5713d56410ba5dac6bf4a8f25f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: CRDWCL01169\n",
      "  info:\n",
      "    learner:\n",
      "      ppo_policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5847749710083008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004859655164182186\n",
      "        model: {}\n",
      "        policy_loss: -0.010886331088840961\n",
      "        total_loss: 8.433229446411133\n",
      "        vf_explained_var: 0.31283825635910034\n",
      "        vf_loss: 8.441929817199707\n",
      "      ppo_policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6293184161186218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00978801492601633\n",
      "        model: {}\n",
      "        policy_loss: -0.012385740876197815\n",
      "        total_loss: 8.428625106811523\n",
      "        vf_explained_var: 0.2993730306625366\n",
      "        vf_loss: 8.436606407165527\n",
      "    num_steps_sampled: 40044\n",
      "    num_steps_trained: 40044\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.92.120.113\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58157894736841\n",
      "    ram_util_percent: 66.2973684210526\n",
      "  pid: 18668\n",
      "  policy_reward_max:\n",
      "    ppo_policy_1: 7.0\n",
      "    ppo_policy_2: 7.0\n",
      "  policy_reward_mean:\n",
      "    ppo_policy_1: -0.14990328820116053\n",
      "    ppo_policy_2: 0.14990328820116053\n",
      "  policy_reward_min:\n",
      "    ppo_policy_1: -7.0\n",
      "    ppo_policy_2: -7.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4844524580813267\n",
      "    mean_inference_ms: 4.498451445960409\n",
      "    mean_processing_ms: 2.569377298308451\n",
      "  time_since_restore: 289.82929968833923\n",
      "  time_this_iter_s: 25.999420642852783\n",
      "  time_total_s: 289.82929968833923\n",
      "  timers:\n",
      "    learn_throughput: 318.913\n",
      "    learn_time_ms: 12556.384\n",
      "    load_throughput: 21907.667\n",
      "    load_time_ms: 182.785\n",
      "    sample_throughput: 257.807\n",
      "    sample_time_ms: 15532.565\n",
      "    update_time_ms: 22.41\n",
      "  timestamp: 1605684593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40044\n",
      "  training_iteration: 10\n",
      "  trial_id: ec3d3_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:29:57,402\tWARNING util.py:137 -- The `process_trial` operation took 0.595120906829834 seconds to complete, which may be a performance bottleneck.\n",
      "2020-11-18 08:29:57,490\tERROR trial_runner.py:350 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 348, in step\n",
      "    self.checkpoint()\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>RUNNING </td><td>10.92.120.113:18668</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         289.829</td><td style=\"text-align: right;\">40044</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 08:29:58,068\tWARNING worker.py:1047 -- A worker died or was killed while executing task ffffffffffffffff4925f4810100.\n",
      "2020-11-18 08:29:58,608\tERROR tune.py:334 -- Trial Runner checkpointing failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\tune.py\", line 332, in run\n",
      "    runner.checkpoint(force=True)\n",
      "  File \"c:\\users\\chiappal\\appdata\\local\\continuum\\miniconda3\\envs\\rl\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 279, in checkpoint\n",
      "    os.rename(tmp_file_name, self.checkpoint_file)\n",
      "FileExistsError: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\.tmp_checkpoint' -> 'C:\\\\Users\\\\chiappal\\\\Documents\\\\rl_project\\\\rlcard\\\\outputs\\\\2020-11-18\\\\experiment_state-2020-11-18_08-23-15.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/4.0 GiB heap, 0.0/1.37 GiB objects<br>Result logdir: C:\\Users\\chiappal\\Documents\\rl_project\\rlcard\\outputs\\2020-11-18<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_leduc-holdem_ec3d3_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         289.829</td><td style=\"text-align: right;\">40044</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = tune.run(\n",
    "    trainer_class,\n",
    "    name=\"2020-11-18\",  # This is used to specify the logging directory.\n",
    "    stop={\n",
    "        \"training_iteration\": 10,\n",
    "#         \"episodes_total\": 10000\n",
    "    },\n",
    "    verbose=100,\n",
    "    config=trainer_config,\n",
    "    local_dir=\"./outputs\",\n",
    "    checkpoint_freq=10,\n",
    "    checkpoint_at_end=True,\n",
    "    restore=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
